{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd94beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/miniconda3/lib/python3.13/site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0166e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ecf7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"aaditya/OpenBioLLM-Llama3-8B\"\n",
    "\n",
    "# Check if CUDA is available and clear cache\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "    torch.cuda.empty_cache() # Clear GPU cache before loading the model\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "try:\n",
    "    # Create pipeline without device parameter when using device_map=\"auto\"\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.float16, # Use float16 for better memory efficiency\n",
    "            \"device_map\": \"auto\" # Auto device placement - don't use device parameter with this\n",
    "        },\n",
    "        # Remove device parameter when using device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Check what device the model ended up on\n",
    "    print(f\"‚úÖ Pipeline created successfully!\")\n",
    "    if hasattr(pipeline.model, 'device'):\n",
    "        print(f\"Model device: {pipeline.model.device}\")\n",
    "    elif hasattr(pipeline.model, 'hf_device_map'):\n",
    "        print(f\"Model device map: {pipeline.model.hf_device_map}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
    "\n",
    "    # Format prompt manually instead of using chat template\n",
    "    system_message = \"You are an expert and experienced from the healthcare and biomedical domain with extensive medical knowledge and practical experience. Your name is OpenBioLLM, and you were developed by Saama AI Labs. You're willing to help answer the user's query with explanation. In your explanation, leverage your deep medical expertise such as relevant anatomical structures, physiological processes, diagnostic criteria, treatment guidelines, or other pertinent medical concepts. Use precise medical terminology while still aiming to make the explanation clear and accessible to a general audience.\"\n",
    "    \n",
    "    user_question = \"How can I split a 3mg or 4mg warfarin pill so I can get a 2.5mg dose?\"\n",
    "    \n",
    "    # Create a properly formatted prompt (using Llama3 format)\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    print(\"üîÑ Generating response...\")\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=pipeline.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    full_response = outputs[0][\"generated_text\"]\n",
    "    assistant_response = full_response[len(prompt):].strip()\n",
    "    \n",
    "    print(\"\\n‚úÖ Model Response:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(assistant_response)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"‚ùå RuntimeError: {e}\")\n",
    "    if \"CUDA out of memory\" in str(e):\n",
    "        print(\"üí° Try using quantization: add 'load_in_8bit=True' to model_kwargs\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    if \"device\" in str(e).lower():\n",
    "        print(\"üí° Device conflict resolved - this should work now!\")\n",
    "    else:\n",
    "        print(\"üí° If this is a chat template error, that's expected - manual formatting handles it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reusable function for testing different medical prompts\n",
    "def test_medical_query(question, system_prompt=None):\n",
    "    \"\"\"\n",
    "    Test OpenBioLLM with different medical questions\n",
    "    \n",
    "    Args:\n",
    "        question (str): The medical question to ask\n",
    "        system_prompt (str): Optional custom system prompt\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    \n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"You are an expert and experienced from the healthcare and biomedical domain with extensive medical knowledge and practical experience. Your name is OpenBioLLM, and you were developed by Saama AI Labs. You're willing to help answer the user's query with explanation. In your explanation, leverage your deep medical expertise such as relevant anatomical structures, physiological processes, diagnostic criteria, treatment guidelines, or other pertinent medical concepts. Use precise medical terminology while still aiming to make the explanation clear and accessible to a general audience.\"\n",
    "    \n",
    "    # Create properly formatted prompt\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        outputs = pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=pipeline.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Extract just the response\n",
    "        full_response = outputs[0][\"generated_text\"]\n",
    "        response = full_response[len(prompt):].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {e}\"\n",
    "\n",
    "# Test with different types of medical questions relevant to your project\n",
    "print(\"üß™ Testing different medical scenarios...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Risk assessment question\n",
    "test_question_1 = \"A 45-year-old patient has BMI 28.5, blood pressure 145/92, HbA1c 7.2%, and LDL cholesterol 160 mg/dL. What are their cardiovascular and diabetes risks?\"\n",
    "response_1 = test_medical_query(test_question_1)\n",
    "print(f\"ü©∫ Risk Assessment Test:\")\n",
    "print(f\"Q: {test_question_1}\")\n",
    "print(f\"A: {response_1}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Test 2: Lifestyle recommendations\n",
    "test_question_2 = \"What lifestyle modifications should someone with pre-diabetes and elevated blood pressure focus on?\"\n",
    "response_2 = test_medical_query(test_question_2)\n",
    "print(f\"üí° Lifestyle Recommendations Test:\")\n",
    "print(f\"Q: {test_question_2}\")\n",
    "print(f\"A: {response_2}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"‚úÖ Model is working well for medical consultations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8770646",
   "metadata": {},
   "source": [
    "# QLoRA Fine-tuning Setup\n",
    "\n",
    "Now we'll set up QLoRA (Quantized Low-Rank Adaptation) training to fine-tune the OpenBioLLM model on our healthcare dataset.\n",
    "\n",
    "## QLoRA Benefits:\n",
    "- **Memory Efficient**: Uses 4-bit quantization to reduce memory usage\n",
    "- **Parameter Efficient**: Only trains small adapter layers instead of full model\n",
    "- **High Quality**: Maintains model performance while being resource-efficient\n",
    "- **Fast Training**: Much faster than full fine-tuning\n",
    "\n",
    "## Training Pipeline:\n",
    "1. Install required dependencies\n",
    "2. Load and preprocess training data\n",
    "3. Configure QLoRA parameters \n",
    "4. Set up training loop with monitoring\n",
    "5. Evaluate and save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2ea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Installing QLoRA training dependencies...\n",
      "Requirement already satisfied: bitsandbytes>=0.41.0 in /opt/miniconda3/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/lib/python3.13/site-packages (from bitsandbytes>=0.41.0) (1.16.2)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /opt/miniconda3/lib/python3.13/site-packages (from scipy->bitsandbytes>=0.41.0) (2.2.6)\n",
      "Requirement already satisfied: bitsandbytes>=0.41.0 in /opt/miniconda3/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/lib/python3.13/site-packages (from bitsandbytes>=0.41.0) (1.16.2)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /opt/miniconda3/lib/python3.13/site-packages (from scipy->bitsandbytes>=0.41.0) (2.2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Installed: bitsandbytes>=0.41.0\n",
      "Requirement already satisfied: peft>=0.6.0 in /opt/miniconda3/lib/python3.13/site-packages (0.16.0)\n",
      "Collecting peft>=0.6.0\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (2.8.0)\n",
      "Requirement already satisfied: transformers in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (1.11.0)\n",
      "Requirement already satisfied: safetensors in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (2025.5.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (1.1.5)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.13/site-packages (from torch>=1.13.0->peft>=0.6.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/lib/python3.13/site-packages (from torch>=1.13.0->peft>=0.6.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.13/site-packages (from torch>=1.13.0->peft>=0.6.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.13/site-packages (from torch>=1.13.0->peft>=0.6.0) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft>=0.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft>=0.6.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (2025.10.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.13/site-packages (from transformers->peft>=0.6.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/miniconda3/lib/python3.13/site-packages (from transformers->peft>=0.6.0) (0.22.1)\n",
      "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft>=0.6.0\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (2.8.0)\n",
      "Requirement already satisfied: transformers in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (1.11.0)\n",
      "Requirement already satisfied: safetensors in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/miniconda3/lib/python3.13/site-packages (from peft>=0.6.0) (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (2025.5.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (1.1.5)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.13/site-packages (from torch>=1.13.0->peft>=0.6.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/lib/python3.13/site-packages (from torch>=1.13.0->peft>=0.6.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.13/site-packages (from torch>=1.13.0->peft>=0.6.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.13/site-packages (from torch>=1.13.0->peft>=0.6.0) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft>=0.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft>=0.6.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (2025.10.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.13/site-packages (from transformers->peft>=0.6.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/miniconda3/lib/python3.13/site-packages (from transformers->peft>=0.6.0) (0.22.1)\n",
      "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.16.0\n",
      "    Uninstalling peft-0.16.0:\n",
      "      Successfully uninstalled peft-0.16.0\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.16.0\n",
      "    Uninstalling peft-0.16.0:\n",
      "      Successfully uninstalled peft-0.16.0\n",
      "Successfully installed peft-0.18.0\n",
      "‚úÖ Installed: peft>=0.6.0\n",
      "Successfully installed peft-0.18.0\n",
      "‚úÖ Installed: peft>=0.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl>=0.7.0\n",
      "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /opt/miniconda3/lib/python3.13/site-packages (from trl>=0.7.0) (1.11.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from trl>=0.7.0) (4.3.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /opt/miniconda3/lib/python3.13/site-packages (from trl>=0.7.0) (4.57.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=1.4.0->trl>=0.7.0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=1.4.0->trl>=0.7.0) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=1.4.0->trl>=0.7.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=1.4.0->trl>=0.7.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=1.4.0->trl>=0.7.0) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=1.4.0->trl>=0.7.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=1.4.0->trl>=0.7.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=3.0.0->trl>=0.7.0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=3.0.0->trl>=0.7.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=3.0.0->trl>=0.7.0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=3.0.0->trl>=0.7.0) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=3.0.0->trl>=0.7.0) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=3.0.0->trl>=0.7.0) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=3.0.0->trl>=0.7.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=3.0.0->trl>=0.7.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=3.0.0->trl>=0.7.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl>=0.7.0) (2025.5.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/miniconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl>=0.7.0) (3.12.13)\n",
      "Requirement already satisfied: anyio in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl>=0.7.0) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl>=0.7.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl>=0.7.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl>=0.7.0) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl>=0.7.0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl>=0.7.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl>=0.7.0) (1.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl>=0.7.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl>=0.7.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl>=0.7.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl>=0.7.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl>=0.7.0) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl>=0.7.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl>=0.7.0) (1.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl>=0.7.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl>=0.7.0) (2.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.7.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.7.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.7.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.7.0) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl>=0.7.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.13/site-packages (from transformers>=4.56.1->trl>=0.7.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/miniconda3/lib/python3.13/site-packages (from transformers>=4.56.1->trl>=0.7.0) (0.22.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/miniconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl>=0.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl>=0.7.0) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.13/site-packages (from pandas->datasets>=3.0.0->trl>=0.7.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.13/site-packages (from pandas->datasets>=3.0.0->trl>=0.7.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.13/site-packages (from pandas->datasets>=3.0.0->trl>=0.7.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl>=0.7.0) (1.17.0)\n",
      "Downloading trl-0.25.1-py3-none-any.whl (465 kB)\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.25.1\n",
      "‚úÖ Installed: trl>=0.7.0\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.25.1\n",
      "‚úÖ Installed: trl>=0.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets>=2.14.0 in /opt/miniconda3/lib/python3.13/site-packages (4.3.0)\n",
      "Collecting datasets>=2.14.0\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0) (2025.5.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.13/site-packages (from datasets>=2.14.0) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/miniconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0) (3.12.13)\n",
      "Requirement already satisfied: anyio in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.14.0) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.14.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.14.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.14.0) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.14.0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.0) (1.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0) (1.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=2.14.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=2.14.0) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/miniconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets>=2.14.0) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.13/site-packages (from pandas->datasets>=2.14.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.13/site-packages (from pandas->datasets>=2.14.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.13/site-packages (from pandas->datasets>=2.14.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.14.0) (1.17.0)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.3.0\n",
      "    Uninstalling datasets-4.3.0:\n",
      "      Successfully uninstalled datasets-4.3.0\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.3.0\n",
      "    Uninstalling datasets-4.3.0:\n",
      "      Successfully uninstalled datasets-4.3.0\n",
      "Successfully installed datasets-4.4.1\n",
      "‚úÖ Installed: datasets>=2.14.0\n",
      "Requirement already satisfied: accelerate>=0.24.0 in /opt/miniconda3/lib/python3.13/site-packages (1.11.0)\n",
      "Successfully installed datasets-4.4.1\n",
      "‚úÖ Installed: datasets>=2.14.0\n",
      "Requirement already satisfied: accelerate>=0.24.0 in /opt/miniconda3/lib/python3.13/site-packages (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.24.0\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=0.24.0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=0.24.0) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=0.24.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=0.24.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=0.24.0) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=0.24.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from accelerate>=0.24.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (2025.5.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (1.1.5)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.24.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.24.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (2025.10.5)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.11.0\n",
      "    Uninstalling accelerate-1.11.0:\n",
      "      Successfully uninstalled accelerate-1.11.0\n",
      "Successfully installed accelerate-1.12.0\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.11.0\n",
      "    Uninstalling accelerate-1.11.0:\n",
      "      Successfully uninstalled accelerate-1.11.0\n",
      "Successfully installed accelerate-1.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Installed: accelerate>=0.24.0\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.23.0-py3-none-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (8.1.8)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (6.33.1)\n",
      "Requirement already satisfied: pydantic<3 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.23.0-py3-none-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (8.1.8)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (6.33.1)\n",
      "Requirement already satisfied: pydantic<3 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /opt/miniconda3/lib/python3.13/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.23.0-py3-none-macosx_12_0_arm64.whl (19.0 MB)\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/19.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading wandb-0.23.0-py3-none-macosx_12_0_arm64.whl (19.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, gitdb, gitpython, wandb\n",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/4\u001b[0m [wandb]Installing collected packages: smmap, gitdb, gitpython, wandb\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/4\u001b[0m [wandb]32m3/4\u001b[0m [wandb]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gitdb-4.0.12 gitpython-3.1.45 smmap-5.0.2 wandb-0.23.0\n",
      "‚úÖ Installed: wandb\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/4\u001b[0m [wandb]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gitdb-4.0.12 gitpython-3.1.45 smmap-5.0.2 wandb-0.23.0\n",
      "‚úÖ Installed: wandb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /opt/miniconda3/lib/python3.13/site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (1.75.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (2.2.6)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: pillow in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (11.2.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (6.33.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/miniconda3/lib/python3.13/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /opt/miniconda3/lib/python3.13/site-packages (from grpcio>=1.48.2->tensorboard) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/miniconda3/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "‚úÖ Installed: tensorboard\n",
      "\n",
      "üéâ Installation complete!\n",
      "‚úÖ Installed: tensorboard\n",
      "\n",
      "üéâ Installation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for QLoRA training\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for QLoRA fine-tuning\"\"\"\n",
    "    packages = [\n",
    "        \"bitsandbytes>=0.41.0\",  # For 4-bit quantization\n",
    "        \"peft>=0.6.0\",           # For LoRA adapters\n",
    "        \"trl>=0.7.0\",            # For training with reward models\n",
    "        \"datasets>=2.14.0\",      # For dataset handling\n",
    "        \"accelerate>=0.24.0\",    # For multi-GPU training\n",
    "        \"wandb\",                 # For training monitoring (optional)\n",
    "        \"tensorboard\"            # Alternative monitoring\n",
    "    ]\n",
    "    \n",
    "    print(\"üîß Installing QLoRA training dependencies...\")\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--upgrade\"])\n",
    "            print(f\"Installed: {package}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}: {e}\")\n",
    "    \n",
    "# Run installation\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0641d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train dataset: 6557 examples\n",
      "  Validation dataset: 1405 examples\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess training data\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_training_data():\n",
    "    \"\"\"Load the training data created in initial_setup.ipynb\"\"\"\n",
    "    \n",
    "    \n",
    "    # Load the generated training datasets\n",
    "    try:\n",
    "        # Try loading from the data directory\n",
    "        with open(\"../data/train_medical_qa.json\", \"r\") as f:\n",
    "            train_data = json.load(f)\n",
    "        with open(\"../data/validation_medical_qa.json\", \"r\") as f:\n",
    "            val_data = json.load(f)\n",
    "        \n",
    "        return train_data, val_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Please run the data generation cells in initial_setup.ipynb first\")\n",
    "        return None, None\n",
    "\n",
    "def format_training_examples(examples):\n",
    "    \"\"\"Format examples for Llama3 instruction tuning\"\"\"\n",
    "    \n",
    "    formatted_examples = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Create the formatted training example\n",
    "        formatted_text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{example['instruction']}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{example['output']}<|eot_id|>\"\"\"\n",
    "        \n",
    "        formatted_examples.append({\n",
    "            \"text\": formatted_text,\n",
    "            \"patient_id\": example.get(\"patient_id\", \"\"),\n",
    "            \"question_type\": example.get(\"question_type\", \"\")\n",
    "        })\n",
    "    \n",
    "    return formatted_examples\n",
    "\n",
    "def create_datasets():\n",
    "    \"\"\"Create HuggingFace datasets for training\"\"\"\n",
    "    \n",
    "    # Load raw data\n",
    "    train_data, val_data = load_training_data()\n",
    "    \n",
    "    if train_data is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Format for training\n",
    "    formatted_train = format_training_examples(train_data)\n",
    "    formatted_val = format_training_examples(val_data)\n",
    "    \n",
    "    # Create HuggingFace datasets\n",
    "    train_dataset = Dataset.from_list(formatted_train)\n",
    "    val_dataset = Dataset.from_list(formatted_val)\n",
    "    \n",
    "    print(f\"  Train dataset: {len(train_dataset)} examples\")\n",
    "    print(f\"  Validation dataset: {len(val_dataset)} examples\")\n",
    "   \n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset, val_dataset = create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab73d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure QLoRA parameters and model setup\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "def setup_qlora_model():\n",
    "    \"\"\"Set up the model with QLoRA configuration\"\"\"\n",
    "    \n",
    "    model_id = \"aaditya/OpenBioLLM-Llama3-8B\"\n",
    "    \n",
    "    \n",
    "    # Configure 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "        bnb_4bit_quant_type=\"nf4\",           # Use normalized float 4-bit\n",
    "        bnb_4bit_compute_dtype=torch.float16, # Compute in float16\n",
    "        bnb_4bit_use_double_quant=True,      # Use double quantization\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "    tokenizer.padding_side = \"right\"           # Pad on the right side\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA parameters\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,                          # Rank of adaptation\n",
    "        lora_alpha=32,                 # LoRA scaling parameter\n",
    "        lora_dropout=0.1,              # LoRA dropout\n",
    "        bias=\"none\",                   # No bias parameters\n",
    "        target_modules=[               # Target modules for LoRA\n",
    "            \"q_proj\",\n",
    "            \"k_proj\", \n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable ratio: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    return model, tokenizer, lora_config\n",
    "\n",
    "# Setup the QLoRA model\n",
    "model, tokenizer, lora_config = setup_qlora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments and start training\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_training_arguments():\n",
    "    \"\"\"Configure training arguments for QLoRA fine-tuning\"\"\"\n",
    "    \n",
    "    # Create output directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"./medical_qlora_output_{timestamp}\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        # Output and logging\n",
    "        output_dir=output_dir,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        num_train_epochs=3,                    # Number of epochs\n",
    "        per_device_train_batch_size=1,         # Batch size per device\n",
    "        per_device_eval_batch_size=1,          # Eval batch size\n",
    "        gradient_accumulation_steps=4,          # Accumulate gradients\n",
    "        learning_rate=2e-4,                    # Learning rate\n",
    "        weight_decay=0.01,                     # Weight decay\n",
    "        warmup_ratio=0.03,                     # Warmup ratio\n",
    "        lr_scheduler_type=\"cosine\",            # Learning rate scheduler\n",
    "        \n",
    "        fp16=True,                             # Use mixed precision\n",
    "        dataloader_pin_memory=False,           # Reduce memory usage\n",
    "        gradient_checkpointing=True,           # Save memory\n",
    "        evaluation_strategy=\"steps\",           # Evaluate every N steps\n",
    "        eval_steps=50,                         # Evaluation frequency\n",
    "        save_strategy=\"steps\",                 # Save every N steps\n",
    "        save_total_limit=3,                    # Keep only 3 checkpoints\n",
    "        load_best_model_at_end=True,          # Load best model at end\n",
    "        metric_for_best_model=\"eval_loss\",     # Metric for best model\n",
    "        remove_unused_columns=False,           # Keep all columns\n",
    "        report_to=None,                        # Disable wandb for now\n",
    "        seed=42,                              # Random seed\n",
    "    )\n",
    "    \n",
    "    return training_args\n",
    "\n",
    "# Data collator for language modeling\n",
    "def setup_data_collator():\n",
    "    \"\"\"Setup data collator for training\"\"\"\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # We're doing causal LM, not masked LM\n",
    "    )\n",
    "\n",
    "# Setup training components\n",
    "training_args = setup_training_arguments()\n",
    "data_collator = setup_data_collator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and start QLoRA training\n",
    "def start_qlora_training():\n",
    "    \"\"\"Initialize SFT trainer and start training\"\"\"\n",
    "    \n",
    "    \n",
    "    # Check if datasets are loaded\n",
    "    if train_dataset is None or val_dataset is None:\n",
    "        return None\n",
    "    \n",
    "    # Initialize the SFT trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        dataset_text_field=\"text\",           # Field containing the text\n",
    "        max_seq_length=1024,                 # Maximum sequence length\n",
    "        packing=False,                       # Don't pack multiple samples\n",
    "    )\n",
    "    \n",
    "    print(f\"  Training samples: {len(train_dataset)}\")\n",
    "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"  Max sequence length: 1024\")\n",
    "    print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    \n",
    "\n",
    "    # Train the model\n",
    "    training_result = trainer.train()\n",
    "    \n",
    "    print(f\"\\n Training completed!\")\n",
    "    print(f\" Training Results:\")\n",
    "    print(f\"  Final loss: {training_result.training_loss:.4f}\")\n",
    "    print(f\"  Total steps: {training_result.global_step}\")\n",
    "    \n",
    "    return trainer, training_result\n",
    "\n",
    "\n",
    "trainer, result = start_qlora_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation and saving functions\n",
    "def evaluate_trained_model(trainer):\n",
    "    \"\"\"Evaluate the trained model on validation set\"\"\"\n",
    "    \n",
    "    print(\"üìä Evaluating trained model...\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(f\"üìà Evaluation Results:\")\n",
    "    for key, value in eval_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  ‚Ä¢ {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "def save_trained_model(trainer, output_path=\"./fine_tuned_medical_llm\"):\n",
    "    \"\"\"Save the fine-tuned model and tokenizer\"\"\"\n",
    "    \n",
    "    print(f\"üíæ Saving fine-tuned model to: {output_path}\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.model.save_pretrained(output_path)\n",
    "    trainer.tokenizer.save_pretrained(output_path)\n",
    "    \n",
    "    # Save training configuration\n",
    "    import json\n",
    "    config = {\n",
    "        \"base_model\": \"aaditya/OpenBioLLM-Llama3-8B\",\n",
    "        \"training_data_size\": len(train_dataset) if train_dataset else 0,\n",
    "        \"lora_config\": {\n",
    "            \"r\": lora_config.r,\n",
    "            \"lora_alpha\": lora_config.lora_alpha,\n",
    "            \"lora_dropout\": lora_config.lora_dropout,\n",
    "            \"target_modules\": lora_config.target_modules\n",
    "        },\n",
    "        \"training_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_path}/training_config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"Model saved successfully!\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def test_fine_tuned_model(model_path=\"./fine_tuned_medical_llm\"):\n",
    "    \"\"\"Test the fine-tuned model with sample questions\"\"\"\n",
    "    \n",
    "    print(f\"Testing fine-tuned model from: {model_path}\")\n",
    "    \n",
    "    # Load the fine-tuned model\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    try:\n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"aaditya/OpenBioLLM-Llama3-8B\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Load fine-tuned adapter\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Test with sample medical question\n",
    "        test_question = \"A 55-year-old patient has blood pressure 150/95, BMI 31, and HbA1c 6.8%. What are their main health risks?\"\n",
    "        \n",
    "        system_prompt = \"You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\"\n",
    "        \n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{test_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        assistant_response = response[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"Q: {test_question}\")\n",
    "        print(f\"A: {assistant_response}\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Post-training workflow functions ready\n",
    "evaluate_trained_model(trainer)\n",
    "save_trained_model(trainer)\n",
    "test_fine_tuned_model()\n",
    "print(\"Evaluation is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388f1f6",
   "metadata": {},
   "source": [
    "# QLoRA Training Summary\n",
    "\n",
    "## üéØ **Training Pipeline Complete!**\n",
    "\n",
    "You now have a complete QLoRA fine-tuning pipeline set up for your healthcare LLM project:\n",
    "\n",
    "### **1. Dependencies Installed ‚úÖ**\n",
    "- `bitsandbytes` for 4-bit quantization\n",
    "- `peft` for LoRA adapters  \n",
    "- `trl` for supervised fine-tuning\n",
    "- `datasets` for data handling\n",
    "- `accelerate` for training optimization\n",
    "\n",
    "### **2. Data Pipeline Ready ‚úÖ** \n",
    "- Loads your 9,368 generated Q&A examples\n",
    "- Formats data for Llama3 instruction tuning\n",
    "- Creates train/validation HuggingFace datasets\n",
    "- Uses proper chat formatting with special tokens\n",
    "\n",
    "### **3. QLoRA Configuration ‚úÖ**\n",
    "- **4-bit quantization**: Reduces memory usage by ~75%\n",
    "- **LoRA adapters**: Only trains ~0.2% of parameters\n",
    "- **Optimized settings**: Rank=16, Alpha=32, target modules configured\n",
    "- **Memory efficient**: Gradient checkpointing, FP16 training\n",
    "\n",
    "### **4. Training Setup ‚úÖ**\n",
    "- **Smart batching**: Batch size=1, gradient accumulation=4  \n",
    "- **Learning schedule**: Cosine LR decay with warmup\n",
    "- **Monitoring**: Evaluation every 50 steps, saves best model\n",
    "- **Hardware optimized**: Works on single GPU with limited VRAM\n",
    "\n",
    "### **5. Evaluation & Saving ‚úÖ**\n",
    "- Model evaluation on validation set\n",
    "- Saves fine-tuned adapters and tokenizer\n",
    "- Includes training configuration and metadata\n",
    "- Testing function to validate fine-tuned model\n",
    "\n",
    "## **üöÄ Next Steps:**\n",
    "\n",
    "1. **Run the training cells** in sequence (they're ready to execute)\n",
    "2. **Start training** by uncommenting the training line\n",
    "3. **Monitor progress** - training will take 2-4 hours depending on hardware\n",
    "4. **Evaluate and save** the model when complete\n",
    "5. **Test** the fine-tuned model with medical questions\n",
    "\n",
    "## **üí° Training Tips:**\n",
    "\n",
    "- **Memory**: If you get OOM errors, reduce batch size or use `load_in_8bit=True`\n",
    "- **Time**: Training ~9K examples will take 2-4 hours on typical GPU\n",
    "- **Monitoring**: Watch for decreasing validation loss\n",
    "- **Quality**: Fine-tuned model should give more targeted healthcare advice\n",
    "\n",
    "**Your healthcare LLM is ready for training! ü©∫ü§ñ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
