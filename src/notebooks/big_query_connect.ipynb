{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a01cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting flexible package installation...\n",
      "ðŸ”§ Installing packages with flexible compatibility approach...\n",
      "ðŸ“¦ Updating pip...\n",
      "âœ… Pip updated successfully\n",
      "ðŸ“¦ Installing packages individually for better compatibility...\n",
      "   Installing numpy...\n",
      "âœ… Pip updated successfully\n",
      "ðŸ“¦ Installing packages individually for better compatibility...\n",
      "   Installing numpy...\n",
      "   âœ… numpy\n",
      "   Installing pandas...\n",
      "   âœ… numpy\n",
      "   Installing pandas...\n",
      "   âœ… pandas\n",
      "   Installing scipy...\n",
      "   âœ… pandas\n",
      "   Installing scipy...\n",
      "   âœ… scipy\n",
      "   Installing scikit-learn...\n",
      "   âœ… scipy\n",
      "   Installing scikit-learn...\n",
      "   âœ… scikit-learn\n",
      "   Installing google-cloud-bigquery...\n",
      "   âœ… scikit-learn\n",
      "   Installing google-cloud-bigquery...\n",
      "   âœ… google-cloud-bigquery\n",
      "   Installing google-cloud-storage...\n",
      "   âœ… google-cloud-bigquery\n",
      "   Installing google-cloud-storage...\n",
      "   âœ… google-cloud-storage\n",
      "   Installing pyarrow...\n",
      "   âœ… google-cloud-storage\n",
      "   Installing pyarrow...\n",
      "   âœ… pyarrow\n",
      "   Installing db-dtypes...\n",
      "   âœ… pyarrow\n",
      "   Installing db-dtypes...\n",
      "   âœ… db-dtypes\n",
      "   Installing torch...\n",
      "   âœ… db-dtypes\n",
      "   Installing torch...\n",
      "   âœ… torch\n",
      "   Installing transformers...\n",
      "   âœ… torch\n",
      "   Installing transformers...\n",
      "   âœ… transformers\n",
      "   Installing datasets...\n",
      "   âœ… transformers\n",
      "   Installing datasets...\n",
      "   âœ… datasets\n",
      "   Installing peft...\n",
      "   âœ… datasets\n",
      "   Installing peft...\n",
      "   âœ… peft\n",
      "   Installing accelerate...\n",
      "   âœ… peft\n",
      "   Installing accelerate...\n",
      "   âœ… accelerate\n",
      "   Installing trl...\n",
      "   âœ… accelerate\n",
      "   Installing trl...\n",
      "   âœ… trl\n",
      "   Installing wandb...\n",
      "   âœ… trl\n",
      "   Installing wandb...\n",
      "   âœ… wandb\n",
      "   Installing matplotlib...\n",
      "   âœ… wandb\n",
      "   Installing matplotlib...\n",
      "   âœ… matplotlib\n",
      "   Installing seaborn...\n",
      "   âœ… matplotlib\n",
      "   Installing seaborn...\n",
      "   âœ… seaborn\n",
      "   Installing tqdm...\n",
      "   âœ… seaborn\n",
      "   Installing tqdm...\n",
      "   âœ… tqdm\n",
      "ðŸ”§ Installing bitsandbytes (may take longer)...\n",
      "   âœ… tqdm\n",
      "ðŸ”§ Installing bitsandbytes (may take longer)...\n",
      "   âœ… bitsandbytes\n",
      "\n",
      "ðŸ“Š Installation Summary:\n",
      "   âœ… Successfully installed: 19 packages\n",
      "   âŒ Failed to install: 0 packages\n",
      "   âœ… Successful: numpy, pandas, scipy, scikit-learn, google-cloud-bigquery, google-cloud-storage, pyarrow, db-dtypes, torch, transformers...\n",
      "\n",
      "ðŸ” Testing critical imports...\n",
      "   âœ… numpy\n",
      "   âœ… bitsandbytes\n",
      "\n",
      "ðŸ“Š Installation Summary:\n",
      "   âœ… Successfully installed: 19 packages\n",
      "   âŒ Failed to install: 0 packages\n",
      "   âœ… Successful: numpy, pandas, scipy, scikit-learn, google-cloud-bigquery, google-cloud-storage, pyarrow, db-dtypes, torch, transformers...\n",
      "\n",
      "ðŸ” Testing critical imports...\n",
      "   âœ… numpy\n",
      "   âœ… pandas\n",
      "   âœ… pandas\n",
      "   âœ… google.cloud.bigquery\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Core functionality should work with 3 critical packages working!\n",
      "\n",
      "âœ… Installation complete! Ready for BigQuery connection and training!\n",
      "   âœ… google.cloud.bigquery\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Core functionality should work with 3 critical packages working!\n",
      "\n",
      "âœ… Installation complete! Ready for BigQuery connection and training!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages_flexibly():\n",
    "    \"\"\"Install packages with flexible version handling and error recovery\"\"\"\n",
    "    \n",
    "    \n",
    "    # Step 1: Upgrade pip first\n",
    "    try:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"], \n",
    "                      capture_output=True, text=True, check=True)\n",
    "    except Exception as e:\n",
    "        print(f\"issues: {e}\")\n",
    "    \n",
    "    # Step 2: Install packages one by one with flexible versions\n",
    "    packages = [\n",
    "        # Core packages - let pip choose compatible versions\n",
    "        \"numpy\",\n",
    "        \"pandas\", \n",
    "        \"scipy\",\n",
    "        \"scikit-learn\",\n",
    "        \n",
    "        # BigQuery packages\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-storage\", \n",
    "        \"pyarrow\",\n",
    "        \"db-dtypes\",\n",
    "        \n",
    "        # ML packages\n",
    "        \"torch\",\n",
    "        \"transformers\",\n",
    "        \"datasets\",\n",
    "        \"peft\",\n",
    "        \"accelerate\",\n",
    "        \"trl\",\n",
    "        \n",
    "        # Additional packages\n",
    "        \"wandb\",\n",
    "        \"matplotlib\",\n",
    "        \"seaborn\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "    \n",
    "    successful_installs = []\n",
    "    failed_installs = []\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            print(f\"   Installing {package}...\")\n",
    "            result = subprocess.run(\n",
    "                [sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                capture_output=True, text=True, timeout=120\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                successful_installs.append(package)\n",
    "                print(f\"  {package}\")\n",
    "            else:\n",
    "                failed_installs.append(package)\n",
    "                print(f\"  {package} - installation issues\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_installs.append(package)\n",
    "    \n",
    "    # Step 3: Try alternative installation for failed packages\n",
    "    if failed_installs:\n",
    "        print(f\"\\nðŸ”„ Retrying failed packages with --no-deps flag...\")\n",
    "        for package in failed_installs[:]:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", package], \n",
    "                    capture_output=True, text=True, timeout=60\n",
    "                )\n",
    "                if result.returncode == 0:\n",
    "                    failed_installs.remove(package)\n",
    "                    successful_installs.append(f\"{package} (no-deps)\")\n",
    "                    print(f\" {package} (installed without dependencies)\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Step 4: Install bitsandbytes separately (often problematic)\n",
    "    try:\n",
    "        print(\"Installing bitsandbytes (may take longer)...\")\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"bitsandbytes\"], \n",
    "            capture_output=True, text=True, timeout=300\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            successful_installs.append(\"bitsandbytes\")\n",
    "        else:\n",
    "            failed_installs.append(\"bitsandbytes\")\n",
    "            print(\"  bitsandbytes - may have compatibility issues\")\n",
    "    except:\n",
    "        failed_installs.append(\"bitsandbytes\")\n",
    "        print(\"  bitsandbytes - installation timeout\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n Installation Summary:\")\n",
    "    print(f\"  Successfully installed: {len(successful_installs)} packages\")\n",
    "    print(f\"  Failed to install: {len(failed_installs)} packages\")\n",
    "    \n",
    "    if successful_installs:\n",
    "        print(f\"  Successful: {', '.join(successful_installs[:10])}...\")\n",
    "    \n",
    "    if failed_installs:\n",
    "        print(f\"  Failed: {', '.join(failed_installs)}\")\n",
    "        print(f\"  These packages may work without them or can be installed later\")\n",
    "    \n",
    "    # Test critical imports\n",
    "    critical_packages = [\"numpy\", \"pandas\", \"google.cloud.bigquery\"]\n",
    "    working_packages = []\n",
    "    \n",
    "    for package in critical_packages:\n",
    "        try:\n",
    "            if package == \"google.cloud.bigquery\":\n",
    "                from google.cloud import bigquery\n",
    "            else:\n",
    "                __import__(package)\n",
    "            working_packages.append(package)\n",
    "            print(f\"  {package}\")\n",
    "        except ImportError:\n",
    "            print(f\"  {package} - import failed\")\n",
    "    \n",
    "    if len(working_packages) >= 2:\n",
    "        print(f\"\\n SUCCESS! Core functionality should work with {len(working_packages)} critical packages working!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n Only {len(working_packages)} critical packages working - may have limited functionality\")\n",
    "        return False\n",
    "\n",
    "# Run flexible installation\n",
    "install_success = install_packages_flexibly()\n",
    "\n",
    "if install_success:\n",
    "    print(\"\\nInstallation complete! Ready for BigQuery connection and training!\")\n",
    "else:\n",
    "    print(\"\\nPartial installation - some functionality may be limited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b00efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Quick verification of essential packages...\n",
      "âœ… NumPy 2.0.2\n",
      "âœ… Pandas 2.3.3\n",
      "âœ… Google Cloud BigQuery imported successfully\n",
      "âœ… PyTorch 2.9.0+cu126\n",
      "âœ… PyTorch 2.9.0+cu126\n",
      "âœ… Transformers 4.57.2\n",
      "\n",
      "ðŸŽ¯ If you see âœ… for NumPy, Pandas, and BigQuery, you can proceed!\n",
      "   Other packages can be installed later as needed.\n",
      "âœ… Transformers 4.57.2\n",
      "\n",
      "ðŸŽ¯ If you see âœ… for NumPy, Pandas, and BigQuery, you can proceed!\n",
      "   Other packages can be installed later as needed.\n"
     ]
    }
   ],
   "source": [
    "# Quick verification of essential packages\n",
    "# Due to random error after model training\n",
    "def verify_essential_packages():\n",
    "    \"\"\"Verify that essential packages are working\"\"\"\n",
    "    print(\"ðŸ” Quick verification of essential packages...\")\n",
    "    \n",
    "    try:\n",
    "        import numpy as np\n",
    "        print(f\"NumPy {np.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"NumPy import failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        import pandas as pd\n",
    "        print(f\" Pandas {pd.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\" Pandas import failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        from google.cloud import bigquery\n",
    "        print(f\" Google Cloud BigQuery imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\" BigQuery import failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        print(f\" PyTorch {torch.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\" PyTorch import failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        import transformers\n",
    "        print(f\" Transformers {transformers.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\" Transformers import failed: {e}\")\n",
    "    \n",
    "# Run verification\n",
    "verify_essential_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666edb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running numpy compatibility fix...\n",
      "ðŸ”§ Alternative numpy compatibility fix...\n",
      "ðŸ”„ Force reinstalling numpy without binary...\n",
      "âœ… NumPy reinstalled successfully!\n",
      "ðŸ“¦ Installing compatible packages...\n",
      "âœ… NumPy reinstalled successfully!\n",
      "ðŸ“¦ Installing compatible packages...\n",
      "   âœ… pandas>=2.0.0\n",
      "   âœ… pandas>=2.0.0\n",
      "   âœ… scipy>=1.10.0\n",
      "   âœ… scipy>=1.10.0\n",
      "   âœ… google-cloud-bigquery[pandas]\n",
      "   âœ… google-cloud-bigquery[pandas]\n",
      "   âœ… google-cloud-storage\n",
      "   âœ… google-cloud-storage\n",
      "   âœ… pyarrow\n",
      "   âœ… pyarrow\n",
      "   âœ… db-dtypes\n",
      "\n",
      "ðŸ” Testing if fix worked...\n",
      "ðŸ” Testing imports...\n",
      "âœ… NumPy 2.0.2\n",
      "âœ… Pandas 2.3.3\n",
      "âœ… BigQuery import successful\n",
      "ðŸŽ‰ SUCCESS! Numpy compatibility issue resolved!\n",
      "   âœ… db-dtypes\n",
      "\n",
      "ðŸ” Testing if fix worked...\n",
      "ðŸ” Testing imports...\n",
      "âœ… NumPy 2.0.2\n",
      "âœ… Pandas 2.3.3\n",
      "âœ… BigQuery import successful\n",
      "ðŸŽ‰ SUCCESS! Numpy compatibility issue resolved!\n"
     ]
    }
   ],
   "source": [
    "# Alternative numpy fix - force reinstall with no binary\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def fix_numpy_compatibility():\n",
    "    \"\"\"Alternative approach to fix numpy compatibility issues\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”§ Alternative numpy compatibility fix...\")\n",
    "    \n",
    "    # Method 1: Force reinstall numpy with no binary to recompile against current system\n",
    "    try:\n",
    "        print(\"Force reinstalling numpy without binary...\")\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"--no-binary=numpy\", \"numpy\"]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"NumPy reinstalled successfully!\")\n",
    "            \n",
    "            # Now install pandas and other packages\n",
    "            print(\"Installing compatible packages...\")\n",
    "            packages = [\n",
    "                \"pandas>=2.0.0\",\n",
    "                \"scipy>=1.10.0\", \n",
    "                \"google-cloud-bigquery[pandas]\",\n",
    "                \"google-cloud-storage\",\n",
    "                \"pyarrow\",\n",
    "                \"db-dtypes\"\n",
    "            ]\n",
    "            \n",
    "            for package in packages:\n",
    "                try:\n",
    "                    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                                 capture_output=True, text=True, timeout=120)\n",
    "                    print(f\"   {package} installed successfully\")\n",
    "                except:\n",
    "                    print(f\"   {package} - may have warnings\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Method 1 failed: {e}\")\n",
    "    \n",
    "    # Method 2: Simple upgrade approach\n",
    "    try:\n",
    "        print(\"Trying simple upgrade approach...\")\n",
    "        upgrade_packages = [\n",
    "            \"numpy\", \"pandas\", \"scipy\", \n",
    "            \"google-cloud-bigquery\", \"pyarrow\"\n",
    "        ]\n",
    "        \n",
    "        for package in upgrade_packages:\n",
    "            try:\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package], \n",
    "                             capture_output=True, text=True, timeout=60)\n",
    "                print(f\"  Upgraded {package}\")\n",
    "            except:\n",
    "                print(f\" {package} upgrade had warnings\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Method 2 failed: {e}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def test_imports():\n",
    "    \"\"\"Test if the numpy issue is resolved\"\"\"\n",
    "    try:\n",
    "        print(\"ðŸ” Testing imports...\")\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        print(f\"NumPy {np.__version__}\")\n",
    "        print(f\" Pandas {pd.__version__}\")\n",
    "        \n",
    "        # Test the specific functionality that was failing\n",
    "        import google.cloud.bigquery\n",
    "        print(f\" BigQuery import successful\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Import test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the fix\n",
    "fix_success = fix_numpy_compatibility()\n",
    "\n",
    "if fix_success:\n",
    "    print(\"\\n Testing if fix worked...\")\n",
    "    test_success = test_imports()\n",
    "    \n",
    "    if test_success:\n",
    "        print(\"SUCCESS! Numpy compatibility issue resolved!\")\n",
    "    else:\n",
    "        print(\"Fix applied but imports still have issues - kernel restart may be needed\")\n",
    "else:\n",
    "    print(\"Fix attempts failed - manual intervention may be required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a29fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "b64 = \"\"\"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\"\"\"\n",
    "\n",
    "with open(\"service-account.json\", \"wb\") as f:\n",
    "    f.write(base64.b64decode(b64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024bf326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery client created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "creds = service_account.Credentials.from_service_account_file(\"service-account.json\")\n",
    "\n",
    "client = bigquery.Client(credentials=creds, project=creds.project_id)\n",
    "print(\"BigQuery client created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3275057",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"aihc-project-480017\"\n",
    "DATASET_NAME = \"synthetic_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242b415e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'aihc-service@aihc-project-480017.iam.gserviceaccount.com'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.oauth2 import service_account\n",
    "import json\n",
    "\n",
    "creds = json.load(open(\"service-account.json\"))\n",
    "creds[\"client_email\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_bigquery_authenticated():\n",
    "    \"\"\"Load medical data from BigQuery tables with authenticated client\"\"\"\n",
    "    \n",
    "    print(\"Loading medical data from BigQuery...\")\n",
    "    \n",
    "    # Define queries for each table\n",
    "    queries = {\n",
    "        'patients': f\"SELECT * FROM `{PROJECT_ID}.{DATASET_NAME}.patients`\",\n",
    "        'encounters': f\"SELECT * FROM `{PROJECT_ID}.{DATASET_NAME}.encounters`\", \n",
    "        'conditions': f\"SELECT * FROM `{PROJECT_ID}.{DATASET_NAME}.conditions`\",\n",
    "        'medications': f\"SELECT * FROM `{PROJECT_ID}.{DATASET_NAME}.medications`\",\n",
    "        'observations': f\"SELECT * FROM `{PROJECT_ID}.{DATASET_NAME}.observations`\",\n",
    "        'procedures': f\"SELECT * FROM `{PROJECT_ID}.{DATASET_NAME}.procedures`\",\n",
    "        'allergies': f\"SELECT * FROM `{PROJECT_ID}.{DATASET_NAME}.allergies`\",\n",
    "        'careplans': f\"SELECT * FROM `{PROJECT_ID}.{DATASET_NAME}.careplans`\"\n",
    "    }\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for name, query in queries.items():\n",
    "        try:\n",
    "            print(f\"   Loading {name}...\")\n",
    "            df = client.query(query).to_dataframe()\n",
    "            datasets[name] = df\n",
    "            print(f\"  {name}: {len(df):,} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {name}: {e}\")\n",
    "            datasets[name] = pd.DataFrame()\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322a2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loading medical data from BigQuery...\n",
      "   Loading patients...\n",
      "   âœ… patients: 1,171 records\n",
      "   Loading encounters...\n",
      "   âœ… patients: 1,171 records\n",
      "   Loading encounters...\n",
      "   âœ… encounters: 53,346 records\n",
      "   Loading conditions...\n",
      "   âœ… encounters: 53,346 records\n",
      "   Loading conditions...\n",
      "   âœ… conditions: 8,376 records\n",
      "   Loading medications...\n",
      "   âœ… conditions: 8,376 records\n",
      "   Loading medications...\n",
      "   âœ… medications: 42,989 records\n",
      "   Loading observations...\n",
      "   âœ… medications: 42,989 records\n",
      "   Loading observations...\n",
      "   âœ… observations: 299,697 records\n",
      "   Loading procedures...\n",
      "   âœ… observations: 299,697 records\n",
      "   Loading procedures...\n",
      "   âœ… procedures: 34,981 records\n",
      "   Loading allergies...\n",
      "   âœ… procedures: 34,981 records\n",
      "   Loading allergies...\n",
      "   âœ… allergies: 597 records\n",
      "   Loading careplans...\n",
      "   âœ… allergies: 597 records\n",
      "   Loading careplans...\n",
      "   âœ… careplans: 3,483 records\n",
      "\n",
      "ðŸ“‹ Data Loading Summary:\n",
      "   Patients: 1,171\n",
      "   Encounters: 53,346\n",
      "   Conditions: 8,376\n",
      "   Medications: 42,989\n",
      "   Observations: 299,697\n",
      "   Procedures: 34,981\n",
      "   Allergies: 597\n",
      "   Care Plans: 3,483\n",
      "   âœ… careplans: 3,483 records\n",
      "\n",
      "ðŸ“‹ Data Loading Summary:\n",
      "   Patients: 1,171\n",
      "   Encounters: 53,346\n",
      "   Conditions: 8,376\n",
      "   Medications: 42,989\n",
      "   Observations: 299,697\n",
      "   Procedures: 34,981\n",
      "   Allergies: 597\n",
      "   Care Plans: 3,483\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "medical_data = load_data_from_bigquery_authenticated()\n",
    "\n",
    "# Extract individual datasets\n",
    "patients_df = medical_data.get('patients', pd.DataFrame())\n",
    "encounters_df = medical_data.get('encounters', pd.DataFrame()) \n",
    "conditions_df = medical_data.get('conditions', pd.DataFrame())\n",
    "medications_df = medical_data.get('medications', pd.DataFrame())\n",
    "observations_df = medical_data.get('observations', pd.DataFrame())\n",
    "procedures_df = medical_data.get('procedures', pd.DataFrame())\n",
    "allergies_df = medical_data.get('allergies', pd.DataFrame())\n",
    "careplans_df = medical_data.get('careplans', pd.DataFrame())\n",
    "\n",
    "print(f\"\\ Data Loading Summary:\")\n",
    "print(f\"   Patients: {len(patients_df):,}\")\n",
    "print(f\"   Encounters: {len(encounters_df):,}\")\n",
    "print(f\"   Conditions: {len(conditions_df):,}\")\n",
    "print(f\"   Medications: {len(medications_df):,}\")\n",
    "print(f\"   Observations: {len(observations_df):,}\")\n",
    "print(f\"   Procedures: {len(procedures_df):,}\")\n",
    "print(f\"   Allergies: {len(allergies_df):,}\")\n",
    "print(f\"   Care Plans: {len(careplans_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164a43d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Medical Q&A generators initialized\n",
      " Clinical classifier initialized\n"
     ]
    }
   ],
   "source": [
    "# Medical Q&A Training Data Generator\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define clinical guidelines for classifications\n",
    "CLINICAL_GUIDELINES = {\n",
    "     'bp_categories': {\n",
    "        'Normal': {'sbp': (0, 120), 'dbp': (0, 80)},\n",
    "        'Elevated': {'sbp': (120, 129), 'dbp': (0, 80)},\n",
    "        'Stage 1': {'sbp': (130, 139), 'dbp': (80, 89)},\n",
    "        'Stage 2': {'sbp': (140, 999), 'dbp': (90, 999)}\n",
    "    },\n",
    "    'bmi_categories': {\n",
    "        'Underweight': (0, 18.5),\n",
    "        'Normal': (18.5, 25.0),\n",
    "        'Overweight': (25.0, 30.0),\n",
    "        'Obese': (30.0, 999)\n",
    "    },\n",
    "    'hba1c_categories': {\n",
    "        'Normal': (0, 5.7),\n",
    "        'Prediabetes': (5.7, 6.5),\n",
    "        'Diabetes': (6.5, 999)\n",
    "    },\n",
    "    'ldl_categories': {\n",
    "        'Optimal': (0, 100),\n",
    "        'Near Optimal': (100, 129),\n",
    "        'Borderline High': (130, 159),\n",
    "        'High': (160, 189),\n",
    "        'Very High': (190, 999)\n",
    "    },\n",
    "    'heart_rate_categories': {\n",
    "        'Bradycardia': (0, 60),\n",
    "        'Normal': (60, 100),\n",
    "        'Tachycardia': (100, 999)\n",
    "    },\n",
    "    'spo2_categories': {\n",
    "        'Critical': (0, 90),\n",
    "        'Low': (90, 95),\n",
    "        'Normal': (95, 100),\n",
    "        'Excellent': (100, 101)  # Handle perfect 100% readings\n",
    "    }\n",
    "}\n",
    "\n",
    "## For future cases\n",
    "\n",
    "# Vitals\n",
    "# 'HeartRate': [211, 220045],\n",
    "# 'SysBP': [51, 442, 455, 6701, 220179, 220050],\n",
    "# 'DiasBP': [8368, 8440, 8441, 8555, 220180, 220051],\n",
    "# 'RespRate': [618, 615, 220210, 224690],\n",
    "# 'TempC': [223762, 676],\n",
    "# 'SpO2': [646, 220277],\n",
    "# 'GCS': [198, 220739, 223900, 223901], # Total GCS\n",
    "\n",
    "# # Labs\n",
    "# 'Lactate': [50813, 52442],\n",
    "# 'Creatinine': [50912, 52546],\n",
    "# 'WBC': [51300, 51301],\n",
    "# 'BUN': [51006],\n",
    "# 'pH': [50820, 50831]\n",
    "\n",
    "class PatientProfileBuilder:\n",
    "    def __init__(self, patients_df, observations_df, conditions_df, medications_df):\n",
    "        self.patients_df = patients_df\n",
    "        self.observations_df = observations_df\n",
    "        self.conditions_df = conditions_df\n",
    "        self.medications_df = medications_df\n",
    "        \n",
    "    def extract_patient_observations(self, patient_id):\n",
    "        \"\"\"Extract all observations for a patient and organize by type\"\"\"\n",
    "        patient_obs = self.observations_df[self.observations_df['PATIENT'] == patient_id]\n",
    "        \n",
    "        obs_dict = {}\n",
    "        for _, row in patient_obs.iterrows():\n",
    "            desc = row['DESCRIPTION'].lower()\n",
    "            value = row.get('VALUE', None)\n",
    "            units = row.get('UNITS', '')\n",
    "            \n",
    "            # Categorize observations\n",
    "            if 'height' in desc:\n",
    "                obs_dict['height'] = {'value': value, 'units': units}\n",
    "            elif 'weight' in desc:\n",
    "                obs_dict['weight'] = {'value': value, 'units': units}\n",
    "            elif 'body mass index' in desc or 'bmi' in desc:\n",
    "                obs_dict['bmi'] = {'value': value, 'units': units}\n",
    "            elif 'systolic' in desc or 'blood pressure systolic' in desc:\n",
    "                obs_dict['sbp'] = {'value': value, 'units': units}\n",
    "            elif 'diastolic' in desc or 'blood pressure diastolic' in desc:\n",
    "                obs_dict['dbp'] = {'value': value, 'units': units}\n",
    "            elif 'heart rate' in desc:\n",
    "                obs_dict['heart_rate'] = {'value': value, 'units': units}\n",
    "            elif 'glucose' in desc and 'fasting' in desc:\n",
    "                obs_dict['fasting_glucose'] = {'value': value, 'units': units}\n",
    "            elif 'hemoglobin a1c' in desc or 'hba1c' in desc:\n",
    "                obs_dict['hba1c'] = {'value': value, 'units': units}\n",
    "            elif 'cholesterol' in desc and 'ldl' in desc:\n",
    "                obs_dict['ldl'] = {'value': value, 'units': units}\n",
    "            elif 'cholesterol' in desc and 'hdl' in desc:\n",
    "                obs_dict['hdl'] = {'value': value, 'units': units}\n",
    "            elif 'cholesterol' in desc and ('total' in desc or 'serum' in desc):\n",
    "                obs_dict['total_cholesterol'] = {'value': value, 'units': units}\n",
    "            elif 'triglyceride' in desc:\n",
    "                obs_dict['triglycerides'] = {'value': value, 'units': units}\n",
    "            elif 'oxygen saturation' in desc or 'spo2' in desc:\n",
    "                obs_dict['spo2'] = {'value': value, 'units': units}\n",
    "                \n",
    "        return obs_dict\n",
    "    \n",
    "    def get_patient_conditions(self, patient_id):\n",
    "        \"\"\"Get all conditions for a patient\"\"\"\n",
    "        patient_conditions = self.conditions_df[self.conditions_df['PATIENT'] == patient_id]\n",
    "        return patient_conditions['DESCRIPTION'].tolist()\n",
    "    \n",
    "    def get_patient_medications(self, patient_id):\n",
    "        \"\"\"Get all medications for a patient\"\"\"\n",
    "        patient_meds = self.medications_df[self.medications_df['PATIENT'] == patient_id]\n",
    "        if 'DESCRIPTION' in patient_meds.columns:\n",
    "            return patient_meds['DESCRIPTION'].tolist()\n",
    "        elif 'CODE' in patient_meds.columns:\n",
    "            return patient_meds['CODE'].tolist()\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def calculate_age(self, birthdate_str):\n",
    "        \"\"\"Calculate age from birthdate\"\"\"\n",
    "        try:\n",
    "            birth_year = pd.to_datetime(birthdate_str).year\n",
    "            current_year = datetime.now().year\n",
    "            return current_year - birth_year\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def build_complete_profile(self, patient_id):\n",
    "        \"\"\"Build a complete patient profile with all available data\"\"\"\n",
    "        \n",
    "        # Get basic patient info\n",
    "        patient_info = self.patients_df[self.patients_df['Id'] == patient_id].iloc[0]\n",
    "        \n",
    "        profile = {\n",
    "            'patient_id': patient_id,\n",
    "            'age': self.calculate_age(patient_info.get('BIRTHDATE', '')),\n",
    "            'gender': patient_info.get('GENDER', 'Unknown'),\n",
    "            'race': patient_info.get('RACE', 'Unknown'),\n",
    "            'ethnicity': patient_info.get('ETHNICITY', 'Unknown'),\n",
    "            'observations': self.extract_patient_observations(patient_id),\n",
    "            'conditions': self.get_patient_conditions(patient_id),\n",
    "            'medications': self.get_patient_medications(patient_id)\n",
    "        }\n",
    "        \n",
    "        # Calculate BMI if height/weight available\n",
    "        if 'height' in profile['observations'] and 'weight' in profile['observations']:\n",
    "            try:\n",
    "                height_cm = float(profile['observations']['height']['value'])\n",
    "                weight_kg = float(profile['observations']['weight']['value'])\n",
    "                \n",
    "                # Convert height to meters if needed\n",
    "                height_m = height_cm / 100 if height_cm > 3 else height_cm\n",
    "                bmi = weight_kg / (height_m ** 2)\n",
    "                profile['observations']['calculated_bmi'] = {'value': round(bmi, 1), 'units': 'kg/m2'}\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return profile\n",
    "\n",
    "# CLINICAL CLASSIFIER - Apply medical guidelines to patient data\n",
    "class ClinicalClassifier:\n",
    "    def __init__(self, guidelines):\n",
    "        self.guidelines = guidelines\n",
    "    \n",
    "    def classify_blood_pressure(self, sbp, dbp):\n",
    "        \"\"\"Classify blood pressure according to ACC/AHA guidelines\"\"\"\n",
    "        if sbp is None or dbp is None:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        try:\n",
    "            sbp, dbp = float(sbp), float(dbp)\n",
    "            \n",
    "            for category, ranges in self.guidelines['bp_categories'].items():\n",
    "                if (ranges['sbp'][0] <= sbp < ranges['sbp'][1] and \n",
    "                    ranges['dbp'][0] <= dbp < ranges['dbp'][1]):\n",
    "                    return category\n",
    "            return \"Stage 2\"  # Default for very high values\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "    \n",
    "    def classify_bmi(self, bmi):\n",
    "        \"\"\"Classify BMI according to WHO guidelines\"\"\"\n",
    "        if bmi is None:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        try:\n",
    "            bmi = float(bmi)\n",
    "            for category, (min_val, max_val) in self.guidelines['bmi_categories'].items():\n",
    "                if min_val <= bmi < max_val:\n",
    "                    return category\n",
    "            return \"Obese\"  # Default for very high values\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "    \n",
    "    def classify_hba1c(self, hba1c):\n",
    "        \"\"\"Classify HbA1c according to ADA guidelines\"\"\"\n",
    "        if hba1c is None:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        try:\n",
    "            hba1c = float(hba1c)\n",
    "            for category, (min_val, max_val) in self.guidelines['hba1c_categories'].items():\n",
    "                if min_val <= hba1c < max_val:\n",
    "                    return category\n",
    "            return \"Diabetes\"  # Default for very high values\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "    \n",
    "    def classify_ldl(self, ldl):\n",
    "        \"\"\"Classify LDL cholesterol according to NCEP ATP III guidelines\"\"\"\n",
    "        if ldl is None:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        try:\n",
    "            ldl = float(ldl)\n",
    "            for category, (min_val, max_val) in self.guidelines['ldl_categories'].items():\n",
    "                if min_val <= ldl < max_val:\n",
    "                    return category\n",
    "            return \"Very High\"  # Default for very high values\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "    \n",
    "    def classify_heart_rate(self, heart_rate):\n",
    "        \"\"\"Classify heart rate according to clinical guidelines\"\"\"\n",
    "        if heart_rate is None:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        try:\n",
    "            hr = float(heart_rate)\n",
    "            for category, (min_val, max_val) in self.guidelines['heart_rate_categories'].items():\n",
    "                if min_val <= hr < max_val:\n",
    "                    return category\n",
    "            return \"Tachycardia\"  # Default for very high values\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "    \n",
    "    def classify_spo2(self, spo2):\n",
    "        \"\"\"Classify oxygen saturation according to clinical guidelines\"\"\"\n",
    "        if spo2 is None:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        try:\n",
    "            spo2_val = float(spo2)\n",
    "            for category, (min_val, max_val) in self.guidelines['spo2_categories'].items():\n",
    "                if min_val <= spo2_val < max_val:\n",
    "                    return category\n",
    "            return \"Excellent\"  # Default for perfect readings\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "    \n",
    "    def get_risk_assessment(self, profile):\n",
    "        \"\"\"Generate comprehensive risk assessment for a patient\"\"\"\n",
    "        obs = profile['observations']\n",
    "        \n",
    "        # Extract values\n",
    "        sbp = obs.get('sbp', {}).get('value')\n",
    "        dbp = obs.get('dbp', {}).get('value')\n",
    "        bmi = obs.get('bmi', {}).get('value') or obs.get('calculated_bmi', {}).get('value')\n",
    "        hba1c = obs.get('hba1c', {}).get('value')\n",
    "        ldl = obs.get('ldl', {}).get('value')\n",
    "        heart_rate = obs.get('heart_rate', {}).get('value')\n",
    "        spo2 = obs.get('spo2', {}).get('value')\n",
    "        \n",
    "        # Classify each metric\n",
    "        classifications = {\n",
    "            'bp_category': self.classify_blood_pressure(sbp, dbp),\n",
    "            'bmi_category': self.classify_bmi(bmi),\n",
    "            'hba1c_category': self.classify_hba1c(hba1c),\n",
    "            'ldl_category': self.classify_ldl(ldl),\n",
    "            'heart_rate_category': self.classify_heart_rate(heart_rate),\n",
    "            'spo2_category': self.classify_spo2(spo2)\n",
    "        }\n",
    "        \n",
    "        return classifications\n",
    "\n",
    "class MedicalQAGenerator:\n",
    "    def __init__(self):\n",
    "        self.question_templates = {\n",
    "            'risk_assessment': [\n",
    "                \"What are my health risks based on my recent test results?\",\n",
    "                \"Can you explain my cardiovascular risk factors?\",\n",
    "                \"Am I at risk for diabetes based on my lab values?\",\n",
    "                \"What do my blood pressure readings mean for my health?\",\n",
    "                \"Should I be concerned about my cholesterol levels?\",\n",
    "                \"What does my BMI indicate about my health status?\",\n",
    "                \"Can you assess my overall health risk profile?\",\n",
    "                \"What are the implications of my HbA1c level?\",\n",
    "                \"How do my current health metrics affect my long-term health?\",\n",
    "                \"Is my heart rate normal? What does it indicate about my health?\",\n",
    "                \"Should I be concerned about my oxygen levels?\"\n",
    "            ],\n",
    "            'lifestyle_advice': [\n",
    "                \"What lifestyle changes should I make to improve my health?\",\n",
    "                \"What diet modifications would you recommend for my condition?\",\n",
    "                \"How much exercise should I be doing with my current health status?\",\n",
    "                \"What can I do to lower my blood pressure naturally?\",\n",
    "                \"How can I manage my prediabetes through lifestyle changes?\",\n",
    "                \"What foods should I avoid with my current cholesterol levels?\",\n",
    "                \"Can lifestyle changes help me avoid medication?\",\n",
    "                \"What daily habits would improve my health markers?\",\n",
    "                \"How can I improve my heart rate and cardiovascular fitness?\"\n",
    "            ],\n",
    "            'specific_concerns': [\n",
    "                \"I'm worried about my family history of heart disease. What should I know?\",\n",
    "                \"My doctor mentioned my blood sugar is elevated. What does this mean?\",\n",
    "                \"I've been experiencing some symptoms. Could they be related to my test results?\",\n",
    "                \"Should I be taking medication based on these numbers?\",\n",
    "                \"When should I schedule my next health screening?\",\n",
    "                \"What warning signs should I watch for with my current health status?\",\n",
    "                \"My heart rate seems unusual - is this something to worry about?\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def format_patient_data(self, profile, classifications):\n",
    "        \"\"\"Format patient data for inclusion in conversation\"\"\"\n",
    "        obs = profile['observations']\n",
    "        \n",
    "        data_text = f\"Patient Profile:\\\\n\"\n",
    "        data_text += f\"Age: {profile['age']}, Gender: {profile['gender']}\\\\n\"\n",
    "        \n",
    "        # Add clinical metrics if available\n",
    "        if 'bmi' in obs or 'calculated_bmi' in obs:\n",
    "            bmi_val = obs.get('bmi', {}).get('value') or obs.get('calculated_bmi', {}).get('value')\n",
    "            data_text += f\"BMI: {bmi_val} kg/mÂ² ({classifications['bmi_category']})\\\\n\"\n",
    "        \n",
    "        if 'sbp' in obs and 'dbp' in obs:\n",
    "            sbp = obs['sbp']['value']\n",
    "            dbp = obs['dbp']['value']\n",
    "            data_text += f\"Blood Pressure: {sbp}/{dbp} mmHg ({classifications['bp_category']})\\\\n\"\n",
    "        \n",
    "        if 'heart_rate' in obs:\n",
    "            hr = obs['heart_rate']['value']\n",
    "            data_text += f\"Heart Rate: {hr} bpm ({classifications['heart_rate_category']})\\\\n\"\n",
    "        \n",
    "        if 'spo2' in obs:\n",
    "            spo2 = obs['spo2']['value']\n",
    "            data_text += f\"Oxygen Saturation: {spo2}% ({classifications['spo2_category']})\\\\n\"\n",
    "        \n",
    "        if 'hba1c' in obs:\n",
    "            hba1c = obs['hba1c']['value']\n",
    "            data_text += f\"HbA1c: {hba1c}% ({classifications['hba1c_category']})\\\\n\"\n",
    "        \n",
    "        if 'ldl' in obs:\n",
    "            ldl = obs['ldl']['value']\n",
    "            data_text += f\"LDL Cholesterol: {ldl} mg/dL ({classifications['ldl_category']})\\\\n\"\n",
    "        \n",
    "        if 'hdl' in obs:\n",
    "            hdl = obs['hdl']['value']\n",
    "            data_text += f\"HDL Cholesterol: {hdl} mg/dL\\\\n\"\n",
    "        \n",
    "        if 'fasting_glucose' in obs:\n",
    "            glucose = obs['fasting_glucose']['value']\n",
    "            data_text += f\"Fasting Glucose: {glucose} mg/dL\\\\n\"\n",
    "        \n",
    "        # Add conditions if present\n",
    "        if profile['conditions']:\n",
    "            conditions_text = \", \".join(profile['conditions'][:3])  # Limit to first 3\n",
    "            data_text += f\"Known Conditions: {conditions_text}\\\\n\"\n",
    "        \n",
    "        # Add medications if present\n",
    "        if profile['medications']:\n",
    "            meds_text = \", \".join(profile['medications'][:3])  # Limit to first 3\n",
    "            data_text += f\"Current Medications: {meds_text}\\\\n\"\n",
    "        \n",
    "        return data_text.strip()\n",
    "    \n",
    "    def generate_medical_response(self, profile, classifications, question_type):\n",
    "        \"\"\"Generate appropriate medical response based on patient data and question type\"\"\"\n",
    "        \n",
    "        response = \"Based on your health profile, here are my observations and recommendations:\\\\n\\\\n\"\n",
    "        \n",
    "        # Risk assessment based on classifications\n",
    "        risks = []\n",
    "        if classifications['bp_category'] in ['Stage 1', 'Stage 2']:\n",
    "            risks.append(\"elevated blood pressure\")\n",
    "        if classifications['bmi_category'] in ['Overweight', 'Obese']:\n",
    "            risks.append(\"excess weight\")\n",
    "        if classifications['hba1c_category'] in ['Prediabetes', 'Diabetes']:\n",
    "            risks.append(\"elevated blood glucose\")\n",
    "        if classifications['ldl_category'] in ['High', 'Very High']:\n",
    "            risks.append(\"high cholesterol\")\n",
    "        if classifications['heart_rate_category'] in ['Bradycardia', 'Tachycardia']:\n",
    "            risks.append(\"abnormal heart rate\")\n",
    "        if classifications['spo2_category'] in ['Critical', 'Low']:\n",
    "            risks.append(\"low oxygen saturation\")\n",
    "        \n",
    "        if risks:\n",
    "            response += f\"**Current Risk Factors:** You have {', '.join(risks)}. \"\n",
    "            response += \"These factors can increase your risk of cardiovascular disease and other complications.\\\\n\\\\n\"\n",
    "        \n",
    "        # Lifestyle recommendations\n",
    "        response += \"**Lifestyle Recommendations:**\\\\n\"\n",
    "        \n",
    "        if classifications['bp_category'] != 'Normal':\n",
    "            response += \"â€¢ Focus on reducing sodium intake and increasing physical activity to help manage blood pressure\\\\n\"\n",
    "        \n",
    "        if classifications['bmi_category'] in ['Overweight', 'Obese']:\n",
    "            response += \"â€¢ Consider a structured weight management program with balanced nutrition and regular exercise\\\\n\"\n",
    "        \n",
    "        if classifications['hba1c_category'] in ['Prediabetes', 'Diabetes']:\n",
    "            response += \"â€¢ Monitor carbohydrate intake and consider working with a nutritionist for diabetes management\\\\n\"\n",
    "        \n",
    "        if classifications['ldl_category'] in ['Borderline High', 'High', 'Very High']:\n",
    "            response += \"â€¢ Adopt a heart-healthy diet low in saturated fats and high in fiber\\\\n\"\n",
    "        \n",
    "        if classifications['heart_rate_category'] == 'Bradycardia':\n",
    "            response += \"â€¢ Monitor for symptoms and discuss with your healthcare provider as bradycardia may require evaluation\\\\n\"\n",
    "        elif classifications['heart_rate_category'] == 'Tachycardia':\n",
    "            response += \"â€¢ Consider stress reduction techniques and limit caffeine intake; discuss with your healthcare provider\\\\n\"\n",
    "        \n",
    "        if classifications['spo2_category'] in ['Critical', 'Low']:\n",
    "            response += \"â€¢ Seek immediate medical attention for low oxygen levels - this requires urgent evaluation\\\\n\"\n",
    "        \n",
    "        response += \"â€¢ Regular monitoring and follow-up with your healthcare provider is important\\\\n\\\\n\"\n",
    "        \n",
    "        response += \"**Important Note:** This guidance is for educational purposes. Please consult with your healthcare provider for personalized medical advice and treatment decisions.\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def generate_qa_pair(self, profile, classifications):\n",
    "        \"\"\"Generate a complete question-answer pair for training\"\"\"\n",
    "        \n",
    "        # Select random question type and template\n",
    "        question_type = random.choice(list(self.question_templates.keys()))\n",
    "        question_template = random.choice(self.question_templates[question_type])\n",
    "        \n",
    "        # Format patient data\n",
    "        patient_data = self.format_patient_data(profile, classifications)\n",
    "        \n",
    "        # Create the complete question with patient context\n",
    "        full_question = f\"{patient_data}\\\\n\\\\nPatient Question: {question_template}\"\n",
    "        \n",
    "        # Generate appropriate response\n",
    "        response = self.generate_medical_response(profile, classifications, question_type)\n",
    "        \n",
    "        return {\n",
    "            'instruction': \"You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\",\n",
    "            'input': full_question,\n",
    "            'output': response,\n",
    "            'patient_id': profile['patient_id'],\n",
    "            'question_type': question_type,\n",
    "            'classifications': classifications\n",
    "        }\n",
    "\n",
    "# Initialize the generators\n",
    "profile_builder = PatientProfileBuilder(patients_df, observations_df, conditions_df, medications_df)\n",
    "classifier = ClinicalClassifier(CLINICAL_GUIDELINES)\n",
    "qa_generator = MedicalQAGenerator()\n",
    "\n",
    "print(\" Medical Q&A generators initialized\")\n",
    "print(\" Clinical classifier initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42b1e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generating 5000 training examples...\n",
      " Processing 1171 patients\n",
      "   Processed 100 patients, Generated 400 examples\n",
      "   Processed 100 patients, Generated 400 examples\n",
      "   Processed 200 patients, Generated 800 examples\n",
      "   Processed 200 patients, Generated 800 examples\n",
      "   Processed 300 patients, Generated 1200 examples\n",
      "   Processed 300 patients, Generated 1200 examples\n",
      "   Processed 400 patients, Generated 1600 examples\n",
      "   Processed 400 patients, Generated 1600 examples\n",
      "   Processed 500 patients, Generated 2000 examples\n",
      "   Processed 500 patients, Generated 2000 examples\n",
      "   Processed 600 patients, Generated 2400 examples\n",
      "   Processed 600 patients, Generated 2400 examples\n",
      "   Processed 700 patients, Generated 2800 examples\n",
      "   Processed 700 patients, Generated 2800 examples\n",
      "   Processed 800 patients, Generated 3200 examples\n",
      "   Processed 800 patients, Generated 3200 examples\n",
      "   Processed 900 patients, Generated 3600 examples\n",
      "   Processed 900 patients, Generated 3600 examples\n",
      "   Processed 1000 patients, Generated 4000 examples\n",
      "   Processed 1000 patients, Generated 4000 examples\n",
      "   Processed 1100 patients, Generated 4400 examples\n",
      "   Processed 1100 patients, Generated 4400 examples\n",
      "\n",
      " Dataset Generation Complete!\n",
      "   Total examples: 4684\n",
      "   Patients processed: 1171\n",
      "\n",
      " Dataset Splits:\n",
      "   Training: 3278 (70.0%)\n",
      "   Validation: 702 (15.0%)\n",
      "   Test: 704 (15.0%)\n",
      "\\n Saving datasets to ./data/\n",
      " train: 3278 examples saved\n",
      " validation: 702 examples saved\n",
      " test: 704 examples saved\n",
      " Combined dataset saved\n",
      "\n",
      " Training data generation complete!\n",
      " Ready to proceed with QLoRA training\n",
      "\n",
      " Dataset Generation Complete!\n",
      "   Total examples: 4684\n",
      "   Patients processed: 1171\n",
      "\n",
      " Dataset Splits:\n",
      "   Training: 3278 (70.0%)\n",
      "   Validation: 702 (15.0%)\n",
      "   Test: 704 (15.0%)\n",
      "\\n Saving datasets to ./data/\n",
      " train: 3278 examples saved\n",
      " validation: 702 examples saved\n",
      " test: 704 examples saved\n",
      " Combined dataset saved\n",
      "\n",
      " Training data generation complete!\n",
      " Ready to proceed with QLoRA training\n"
     ]
    }
   ],
   "source": [
    "# Generate Training Dataset\n",
    "import os\n",
    "\n",
    "\n",
    "def generate_training_dataset(target_size=5000):\n",
    "    \"\"\"Generate training dataset from patient data\"\"\"\n",
    "    \n",
    "    print(f\" Generating {target_size} training examples...\")\n",
    "    \n",
    "    # Get all unique patient IDs\n",
    "    unique_patients = patients_df['Id'].unique()\n",
    "    print(f\" Processing {len(unique_patients)} patients\")\n",
    "    \n",
    "    training_examples = []\n",
    "    patients_processed = 0\n",
    "    \n",
    "    # Generate multiple Q&A pairs per patient to reach target size\n",
    "    examples_per_patient = max(1, target_size // len(unique_patients))\n",
    "    \n",
    "    for i, patient_id in enumerate(unique_patients):\n",
    "        try:\n",
    "            # Build complete patient profile\n",
    "            profile = profile_builder.build_complete_profile(patient_id)\n",
    "            \n",
    "            # Skip patients with insufficient data\n",
    "            if not profile['observations'] or len(profile['observations']) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Get clinical classifications\n",
    "            classifications = classifier.get_risk_assessment(profile)\n",
    "            \n",
    "            # Skip if no meaningful classifications\n",
    "            if all(v == 'Unknown' for v in classifications.values()):\n",
    "                continue\n",
    "            \n",
    "            # Generate multiple Q&A pairs for this patient\n",
    "            for _ in range(examples_per_patient):\n",
    "                try:\n",
    "                    qa_pair = qa_generator.generate_qa_pair(profile, classifications)\n",
    "                    training_examples.append(qa_pair)\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            patients_processed += 1\n",
    "            \n",
    "            # Progress update\n",
    "            if patients_processed % 100 == 0:\n",
    "                print(f\"   Processed {patients_processed} patients, Generated {len(training_examples)} examples\")\n",
    "            \n",
    "            # Stop when we reach target size\n",
    "            if len(training_examples) >= target_size:\n",
    "                break\n",
    "                \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n Dataset Generation Complete!\")\n",
    "    print(f\"   Total examples: {len(training_examples)}\")\n",
    "    print(f\"   Patients processed: {patients_processed}\")\n",
    "    \n",
    "    return training_examples\n",
    "\n",
    "# Generate the training dataset\n",
    "training_data = generate_training_dataset(target_size=5000)\n",
    "\n",
    "# Create train/validation/test splits\n",
    "def create_dataset_splits(training_data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"Split data into train/validation/test sets\"\"\"\n",
    "    \n",
    "    random.shuffle(training_data)\n",
    "    \n",
    "    total_size = len(training_data)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    \n",
    "    train_data = training_data[:train_size]\n",
    "    val_data = training_data[train_size:train_size + val_size]\n",
    "    test_data = training_data[train_size + val_size:]\n",
    "    \n",
    "    print(f\"\\n Dataset Splits:\")\n",
    "    print(f\"   Training: {len(train_data)} ({len(train_data)/total_size*100:.1f}%)\")\n",
    "    print(f\"   Validation: {len(val_data)} ({len(val_data)/total_size*100:.1f}%)\")\n",
    "    print(f\"   Test: {len(test_data)} ({len(test_data)/total_size*100:.1f}%)\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Create splits\n",
    "train_split, val_split, test_split = create_dataset_splits(training_data)\n",
    "\n",
    "# Save datasets locally\n",
    "def save_datasets(train_data, val_data, test_data):\n",
    "    \"\"\"Save the datasets to JSON files\"\"\"\n",
    "    \n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "    \n",
    "    datasets = {\n",
    "        'train': train_data,\n",
    "        'validation': val_data, \n",
    "        'test': test_data\n",
    "    }\n",
    "    \n",
    "    print(f\"\\\\n Saving datasets to ./data/\")\n",
    "    \n",
    "    for split_name, data in datasets.items():\n",
    "        filename = f\"./data/{split_name}_medical_qa.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\" {split_name}: {len(data)} examples saved\")\n",
    "    \n",
    "    # Also create a combined dataset file\n",
    "    combined_data = {\n",
    "        'train': train_data,\n",
    "        'validation': val_data,\n",
    "        'test': test_data,\n",
    "        'metadata': {\n",
    "            'total_examples': len(train_data) + len(val_data) + len(test_data),\n",
    "            'train_size': len(train_data),\n",
    "            'val_size': len(val_data),\n",
    "            'test_size': len(test_data),\n",
    "            'generation_date': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(\"./data/combined_medical_qa.json\", 'w') as f:\n",
    "        json.dump(combined_data, f, indent=2)\n",
    "    \n",
    "    print(f\" Combined dataset saved\")\n",
    "    return datasets\n",
    "\n",
    "# Save the datasets\n",
    "saved_datasets = save_datasets(train_split, val_split, test_split)\n",
    "\n",
    "print(\"\\n Training data generation complete!\")\n",
    "print(\" Ready to proceed with QLoRA training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ac88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TRAINING EXAMPLES PREVIEW\n",
      "==================================================\n",
      "\\nðŸ“‹ Example 1:\n",
      "Question Type: lifestyle_advice\n",
      "Patient ID: be8fc232-6d5c-4d97-9a1b-f9b17e06c3ff\n",
      "\\nClinical Classifications:\n",
      "   â€¢ Bp Category: Stage 2\n",
      "   â€¢ Bmi Category: Obese\n",
      "   â€¢ Hba1C Category: Prediabetes\n",
      "   â€¢ Ldl Category: Unknown\n",
      "   â€¢ Heart Rate Category: Normal\n",
      "   â€¢ Spo2 Category: Unknown\n",
      "\\nInstruction: You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\n",
      "\\nInput:\\nPatient Profile:\\nAge: 52, Gender: F\\nBMI: 30.0 kg/mÂ² (Obese)\\nBlood Pressure: 136.0/77.0 mmHg (Stage 2)\\nHeart Rate: 99.0 bpm (Normal)\\nHbA1c: 6.4% (Prediabetes)\\nKnown Conditions: Acute bronchitis (disorder), Blighted ovum, Normal pregnancy\\nCurrent Medications: Acetaminophen 325 MG Oral Tablet, Amlodipine 5 MG Oral Tablet, Amlodipine 5 MG Oral Tablet\\n\\n\\nPatient Question: How much exercise should I be doing with my current health status?\n",
      "\\nOutput:\\nBased on your health profile, here are my observations and recommendations:\\n\\n**Current Risk Factors:** You have elevated blood pressure, excess weight, elevated blood glucose. These factors can increase your risk of cardiovascular disease and other complications.\\n\\n**Lifestyle Recommendations:**\\...\n",
      "\\n--------------------------------------------------\n",
      "\\nðŸ“‹ Example 2:\n",
      "Question Type: specific_concerns\n",
      "Patient ID: cc9b1521-809e-4426-81f0-1d7b307e2184\n",
      "\\nClinical Classifications:\n",
      "   â€¢ Bp Category: Elevated\n",
      "   â€¢ Bmi Category: Underweight\n",
      "   â€¢ Hba1C Category: Unknown\n",
      "   â€¢ Ldl Category: Unknown\n",
      "   â€¢ Heart Rate Category: Normal\n",
      "   â€¢ Spo2 Category: Unknown\n",
      "\\nInstruction: You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\n",
      "\\nInput:\\nPatient Profile:\\nAge: 13, Gender: F\\nBMI: 15.0 kg/mÂ² (Underweight)\\nBlood Pressure: 123.0/79.0 mmHg (Elevated)\\nHeart Rate: 71.0 bpm (Normal)\\nKnown Conditions: Acute bronchitis (disorder), Acute bronchitis (disorder), Viral sinusitis (disorder)\\nCurrent Medications: Acetaminophen 21.7 MG/ML / Dextromethorphan Hydrobromide 1 MG/ML / doxylamine succinate 0.417 MG/ML Oral Solution, Acetaminophen 325 MG Oral Tablet\\n\\n\\nPatient Question: When should I schedule my next health screening?\n",
      "\\nOutput:\\nBased on your health profile, here are my observations and recommendations:\\n\\n**Lifestyle Recommendations:**\\nâ€¢ Focus on reducing sodium intake and increasing physical activity to help manage blood pressure\\nâ€¢ Regular monitoring and follow-up with your healthcare provider is important\\n\\n**Importan...\n",
      "\\n--------------------------------------------------\n",
      "\\nðŸŽ¯ Clinical Classifications Working Properly!\n",
      "âœ… Ready for QLoRA training with medically accurate data\n"
     ]
    }
   ],
   "source": [
    "# Preview Training Examples with Clinical Classifications\n",
    "def preview_training_examples(train_data, num_examples=2):\n",
    "    \"\"\"Show sample training examples with clinical classifications\"\"\"\n",
    "    \n",
    "    for i, example in enumerate(train_data[:num_examples]):\n",
    "        print(f\"\\n Example {i+1}:\")\n",
    "        print(f\"Question Type: {example['question_type']}\")\n",
    "        print(f\"Patient ID: {example['patient_id']}\")\n",
    "        print(f\"\\nClinical Classifications:\")\n",
    "        for metric, category in example['classifications'].items():\n",
    "            print(f\"   â€¢ {metric.replace('_', ' ').title()}: {category}\")\n",
    "        \n",
    "        print(f\"\\nInstruction: {example['instruction']}\")\n",
    "        print(f\"\\nInput:\\n{example['input']}\")\n",
    "        print(f\"\\nOutput:\\n{example['output'][:300]}...\")\n",
    "\n",
    "# Preview examples\n",
    "if training_data:\n",
    "    preview_training_examples(train_split, num_examples=2)\n",
    "else:\n",
    "    print(\"No training data generated yet - run the generation cell above!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e231324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Preparing CORRECTED training datasets...\n",
      "ðŸ”§ Preparing dataset with proper SFT formatting...\n",
      "   âœ… Dataset prepared: 1000 examples\n",
      "   ðŸ“ Sample format check:\n",
      "      Keys: ['text']\n",
      "      Text length: 1441 chars\n",
      "ðŸ”§ Preparing dataset with proper SFT formatting...\n",
      "   âœ… Dataset prepared: 200 examples\n",
      "   ðŸ“ Sample format check:\n",
      "      Keys: ['text']\n",
      "      Text length: 1256 chars\n",
      "\n",
      "âœ… Datasets ready for SFT training:\n",
      "   Training: 1000 examples\n",
      "   Validation: 200 examples\n",
      "\n",
      "ðŸ” Sample training example:\n",
      "Text preview: ### Instruction:\n",
      "You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\n",
      "\n",
      "### Input:\n",
      "Patient Profile:\\nAge: 52, Gender: F\\nBMI: 30.0 kg/mÂ² (O...\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "def format_instruction(sample):\n",
    "    \"\"\"Format the training sample for instruction following\"\"\"\n",
    "    return f\"\"\"### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\"\"\"\n",
    "\n",
    "def prepare_dataset_for_sft(data, max_samples=1000):\n",
    "    \"\"\"Prepare dataset for SFT training with proper formatting\"\"\"\n",
    "    \n",
    "    print(f\" Preparing dataset with proper SFT formatting...\")\n",
    "    \n",
    "    # Limit dataset size for faster training during development\n",
    "    limited_data = data[:max_samples] if len(data) > max_samples else data\n",
    "    \n",
    "    # Format data for SFT - just keep the text field\n",
    "    formatted_data = []\n",
    "    for item in limited_data:\n",
    "        # Create the formatted text\n",
    "        formatted_text = format_instruction(item)\n",
    "        \n",
    "        # SFTTrainer expects simple dictionary with text field\n",
    "        formatted_data.append({\n",
    "            \"text\": formatted_text\n",
    "        })\n",
    "    \n",
    "    # Create Dataset object\n",
    "    dataset = Dataset.from_list(formatted_data)\n",
    "    \n",
    "    print(f\"  Dataset prepared: {len(dataset)} examples\")\n",
    "    print(f\"  Sample format check:\")\n",
    "    print(f\"  Keys: {list(dataset[0].keys())}\")\n",
    "    print(f\"  Text length: {len(dataset[0]['text'])} chars\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Prepare datasets with corrected formatting\n",
    "print(\"  Preparing CORRECTED training datasets...\")\n",
    "train_dataset = prepare_dataset_for_sft(train_split, max_samples=1000)\n",
    "val_dataset = prepare_dataset_for_sft(val_split, max_samples=200)\n",
    "\n",
    "print(f\"\\nDatasets ready for SFT training:\")\n",
    "print(f\"  Training: {len(train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples\")\n",
    "\n",
    "# Verify the first example\n",
    "print(f\"\\n Sample training example:\")\n",
    "print(f\"Text preview: {train_dataset[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "794e2032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up QLoRA training for OpenBioLLM...\n",
      "Loading model: aaditya/OpenBioLLM-Llama3-8B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up QLoRA training for OpenBioLLM...\n",
      "Loading model: aaditya/OpenBioLLM-Llama3-8B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650257ca4b3e4ad59a6c62c689ef2e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up QLoRA training for OpenBioLLM...\n",
      "Loading model: aaditya/OpenBioLLM-Llama3-8B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650257ca4b3e4ad59a6c62c689ef2e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# QLoRA Training Setup\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"ðŸ”§ Setting up QLoRA training for OpenBioLLM...\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"aaditya/OpenBioLLM-Llama3-8B\"\n",
    "# QLoRA configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load the base model and tokenizer\"\"\"\n",
    "    \n",
    "    print(f\"Loading model: {MODEL_NAME}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with 4-bit quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88aee22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401eb14913d42d0b0c9871e61279970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401eb14913d42d0b0c9871e61279970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba892a70cca44479e02cc2fb2be4ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401eb14913d42d0b0c9871e61279970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba892a70cca44479e02cc2fb2be4ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526e7f8a38784da1a63d3141427fc8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401eb14913d42d0b0c9871e61279970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba892a70cca44479e02cc2fb2be4ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526e7f8a38784da1a63d3141427fc8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1969b9aa5e4ed7903a071651774eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401eb14913d42d0b0c9871e61279970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba892a70cca44479e02cc2fb2be4ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526e7f8a38784da1a63d3141427fc8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1969b9aa5e4ed7903a071651774eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20686e9a20d6482dabd6dae4a9cf48f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401eb14913d42d0b0c9871e61279970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba892a70cca44479e02cc2fb2be4ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526e7f8a38784da1a63d3141427fc8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1969b9aa5e4ed7903a071651774eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20686e9a20d6482dabd6dae4a9cf48f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401eb14913d42d0b0c9871e61279970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba892a70cca44479e02cc2fb2be4ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526e7f8a38784da1a63d3141427fc8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1969b9aa5e4ed7903a071651774eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20686e9a20d6482dabd6dae4a9cf48f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SFTTrainer configured with proper tokenization handling\n",
      "ðŸ“Š CORRECTED Training Configuration:\n",
      "   Model: aaditya/OpenBioLLM-Llama3-8B\n",
      "   Training samples: 1,000\n",
      "   Validation samples: 200\n",
      "   Max sequence length: 512\n",
      "   Text field: 'text'\n",
      "   GUARANTEED minimum steps: 1500\n",
      "   Effective batch size: 4\n",
      "   Learning rate: 0.0002\n",
      "   Output directory: ./medical_qlora_corrected_20251204_011656\n",
      "   WANDB tracking: âœ… Enabled\n",
      "\n",
      "ðŸš€ Starting CORRECTED training for MINIMUM 1500 steps...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401eb14913d42d0b0c9871e61279970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba892a70cca44479e02cc2fb2be4ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526e7f8a38784da1a63d3141427fc8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1969b9aa5e4ed7903a071651774eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20686e9a20d6482dabd6dae4a9cf48f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SFTTrainer configured with proper tokenization handling\n",
      "ðŸ“Š CORRECTED Training Configuration:\n",
      "   Model: aaditya/OpenBioLLM-Llama3-8B\n",
      "   Training samples: 1,000\n",
      "   Validation samples: 200\n",
      "   Max sequence length: 512\n",
      "   Text field: 'text'\n",
      "   GUARANTEED minimum steps: 1500\n",
      "   Effective batch size: 4\n",
      "   Learning rate: 0.0002\n",
      "   Output directory: ./medical_qlora_corrected_20251204_011656\n",
      "   WANDB tracking: âœ… Enabled\n",
      "\n",
      "ðŸš€ Starting CORRECTED training for MINIMUM 1500 steps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 2:11:57, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.171929</td>\n",
       "      <td>0.166066</td>\n",
       "      <td>177979.000000</td>\n",
       "      <td>0.952589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.153638</td>\n",
       "      <td>0.156142</td>\n",
       "      <td>353736.000000</td>\n",
       "      <td>0.955625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.144487</td>\n",
       "      <td>0.150017</td>\n",
       "      <td>530930.000000</td>\n",
       "      <td>0.957231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.131700</td>\n",
       "      <td>0.142569</td>\n",
       "      <td>0.129970</td>\n",
       "      <td>708301.000000</td>\n",
       "      <td>0.958041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.138221</td>\n",
       "      <td>0.128570</td>\n",
       "      <td>884202.000000</td>\n",
       "      <td>0.958536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>0.136150</td>\n",
       "      <td>0.123180</td>\n",
       "      <td>1061082.000000</td>\n",
       "      <td>0.959819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.135734</td>\n",
       "      <td>0.117930</td>\n",
       "      <td>1237409.000000</td>\n",
       "      <td>0.960406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.132243</td>\n",
       "      <td>0.116362</td>\n",
       "      <td>1415173.000000</td>\n",
       "      <td>0.961630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.135199</td>\n",
       "      <td>0.109455</td>\n",
       "      <td>1590714.000000</td>\n",
       "      <td>0.962361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.134764</td>\n",
       "      <td>0.109931</td>\n",
       "      <td>1768404.000000</td>\n",
       "      <td>0.962648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing CORRECTED QLoRA training with proper tokenization...\n",
      "ðŸŽ¯ Starting CORRECTED Medical LLM Training...\n",
      "============================================================\n",
      "ðŸš€ Setting up CORRECTED QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikanth-nallamothu\u001b[0m (\u001b[33msrikanth-nallamothu-sophos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251204_011649-medical-qlora-fafec79d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">medical-qlora-fafec79d</a></strong> to <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d' target=\"_blank\">https://wandb.ai/srikanth-nallamothu-sophos/medical-llm-qlora/runs/medical-qlora-fafec79d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WANDB initialized successfully!\n",
      "   Project: medical-llm-qlora\n",
      "   Run ID: medical-qlora-fafec79d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d91357c08465580867017da4d7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401eb14913d42d0b0c9871e61279970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba892a70cca44479e02cc2fb2be4ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526e7f8a38784da1a63d3141427fc8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1969b9aa5e4ed7903a071651774eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20686e9a20d6482dabd6dae4a9cf48f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SFTTrainer configured with proper tokenization handling\n",
      "ðŸ“Š CORRECTED Training Configuration:\n",
      "   Model: aaditya/OpenBioLLM-Llama3-8B\n",
      "   Training samples: 1,000\n",
      "   Validation samples: 200\n",
      "   Max sequence length: 512\n",
      "   Text field: 'text'\n",
      "   GUARANTEED minimum steps: 1500\n",
      "   Effective batch size: 4\n",
      "   Learning rate: 0.0002\n",
      "   Output directory: ./medical_qlora_corrected_20251204_011656\n",
      "   WANDB tracking: âœ… Enabled\n",
      "\n",
      "ðŸš€ Starting CORRECTED training for MINIMUM 1500 steps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 2:11:57, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.171929</td>\n",
       "      <td>0.166066</td>\n",
       "      <td>177979.000000</td>\n",
       "      <td>0.952589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.153638</td>\n",
       "      <td>0.156142</td>\n",
       "      <td>353736.000000</td>\n",
       "      <td>0.955625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.144487</td>\n",
       "      <td>0.150017</td>\n",
       "      <td>530930.000000</td>\n",
       "      <td>0.957231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.131700</td>\n",
       "      <td>0.142569</td>\n",
       "      <td>0.129970</td>\n",
       "      <td>708301.000000</td>\n",
       "      <td>0.958041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.138221</td>\n",
       "      <td>0.128570</td>\n",
       "      <td>884202.000000</td>\n",
       "      <td>0.958536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>0.136150</td>\n",
       "      <td>0.123180</td>\n",
       "      <td>1061082.000000</td>\n",
       "      <td>0.959819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.135734</td>\n",
       "      <td>0.117930</td>\n",
       "      <td>1237409.000000</td>\n",
       "      <td>0.960406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.132243</td>\n",
       "      <td>0.116362</td>\n",
       "      <td>1415173.000000</td>\n",
       "      <td>0.961630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.135199</td>\n",
       "      <td>0.109455</td>\n",
       "      <td>1590714.000000</td>\n",
       "      <td>0.962361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.134764</td>\n",
       "      <td>0.109931</td>\n",
       "      <td>1768404.000000</td>\n",
       "      <td>0.962648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ‰ TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "âœ… Final Training Loss: 0.1657\n",
      "âœ… Total Steps Completed: 1500\n",
      "ðŸŽ¯ SUCCESS: Minimum 1500 steps requirement MET! (1500 steps)\n",
      "ðŸ’¾ Model and tokenizer saved to: ./medical_qlora_corrected_20251204_011656\n",
      "ðŸ“„ Training summary saved\n",
      "ðŸ’¾ Model and tokenizer saved to: ./medical_qlora_corrected_20251204_011656\n",
      "ðŸ“„ Training summary saved\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED QLoRA Training Setup with Proper Tokenization\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def setup_complete_training():\n",
    "    \"\"\"Setup complete training with corrected tokenization and data handling\"\"\"\n",
    "    \n",
    "    print(\" Setting up CORRECTED QLoRA training...\")\n",
    "    \n",
    "    # Setup WANDB with unique run ID\n",
    "    try:\n",
    "        import wandb\n",
    "        \n",
    "        # Generate unique run ID to avoid conflicts\n",
    "        run_id = f\"medical-qlora-{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "        # Finish any existing runs\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish()\n",
    "        \n",
    "        # Initialize new run\n",
    "        wandb_run = wandb.init(\n",
    "            project=\"medical-llm-qlora\",\n",
    "            name=run_id,\n",
    "            id=run_id,\n",
    "            resume=\"never\",\n",
    "            config={\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"task\": \"medical_qa_finetuning\",\n",
    "                \"method\": \"qlora\",\n",
    "                \"dataset_size\": len(train_dataset),\n",
    "                \"max_steps\": 1500,\n",
    "                \"learning_rate\": 2e-4,\n",
    "                \"batch_size\": 4,\n",
    "                \"lora_r\": 16,\n",
    "                \"lora_alpha\": 32,\n",
    "                \"lora_dropout\": 0.1\n",
    "            },\n",
    "            tags=[\"medical\", \"healthcare\", \"qlora\", \"llama3\"],\n",
    "            notes=\"Fine-tuning OpenBioLLM with QLoRA on medical Q&A data - CORRECTED VERSION\"\n",
    "        )\n",
    "        \n",
    "        print(f\" WANDB initialized successfully!\")\n",
    "        print(f\"   Project: medical-llm-qlora\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        wandb_enabled = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" WANDB setup failed: {e}\")\n",
    "        print(\"   Training will continue without WANDB logging\")\n",
    "        wandb_enabled = False\n",
    "\n",
    "    # Create output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"./medical_qlora_corrected_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup training arguments with proper configuration\n",
    "    training_args = TrainingArguments(\n",
    "        # Output and logging\n",
    "        output_dir=output_dir,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=10,\n",
    "        \n",
    "        # Training duration - GUARANTEED MINIMUM 1500 STEPS\n",
    "        max_steps=1500,\n",
    "        num_train_epochs=10,\n",
    "        \n",
    "        # Batch configuration - CORRECTED\n",
    "        per_device_train_batch_size=1,         # Small batch size for memory\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,         # Effective batch size = 4\n",
    "        \n",
    "        # Learning configuration\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        \n",
    "        # Memory optimization\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # Evaluation and saving\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=150,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=150,\n",
    "        save_total_limit=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        \n",
    "        # Data handling - CORRECTED\n",
    "        remove_unused_columns=False,\n",
    "        prediction_loss_only=True,\n",
    "        \n",
    "        # Monitoring\n",
    "        report_to=\"wandb\" if wandb_enabled else \"none\",\n",
    "        run_name=f\"medical-qlora-corrected-{timestamp}\",\n",
    "        \n",
    "        # Reproducibility\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "    )\n",
    "    \n",
    "    # Create SFT trainer with CORRECTED configuration\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return trainer, training_args, wandb_enabled\n",
    "\n",
    "def execute_complete_training():\n",
    "    \n",
    "    try:\n",
    "        # Setup training\n",
    "        trainer, training_args, wandb_enabled = setup_complete_training()\n",
    "        # Execute training with error handling\n",
    "        training_result = trainer.train()\n",
    "        \n",
    "        # Extract and display results\n",
    "        if hasattr(training_result, 'training_loss'):\n",
    "            final_loss = training_result.training_loss\n",
    "            print(f\" Final Training Loss: {final_loss:.4f}\")\n",
    "        \n",
    "        if hasattr(training_result, 'global_step'):\n",
    "            total_steps = training_result.global_step\n",
    "            print(f\" Total Steps Completed: {total_steps}\")\n",
    "            \n",
    "        # Save final model\n",
    "        trainer.save_model()\n",
    "        tokenizer.save_pretrained(training_args.output_dir)\n",
    "        \n",
    "        print(f\" Model and tokenizer saved to: {training_args.output_dir}\")\n",
    "        \n",
    "        # Save training summary\n",
    "        summary = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"training_completed\": True,\n",
    "            \"total_steps\": total_steps if hasattr(training_result, 'global_step') else \"Unknown\",\n",
    "            \"final_loss\": final_loss if hasattr(training_result, 'training_loss') else \"Unknown\",\n",
    "            \"min_steps_met\": total_steps >= 1500 if hasattr(training_result, 'global_step') else False,\n",
    "            \"output_directory\": training_args.output_dir,\n",
    "            \"completion_time\": datetime.now().isoformat(),\n",
    "            \"wandb_enabled\": wandb_enabled,\n",
    "            \"corrections_applied\": [\n",
    "                \"Fixed tokenization handling\",\n",
    "                \"Removed problematic data collator\",\n",
    "                \"Added proper SFTTrainer configuration\",\n",
    "                \"Set max_seq_length and dataset_text_field\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(f\"{training_args.output_dir}/training_summary.json\", 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\" Training summary saved\")\n",
    "        \n",
    "        return trainer, training_result, summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" TRAINING FAILED: {e}\")\n",
    "        print(\" Error details:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# Execute the corrected training\n",
    "final_trainer, final_result, training_summary = execute_complete_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a202781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model evaluation functions ready!\n",
      "ðŸ“‹ Available functions:\n",
      "   â€¢ evaluate_trained_model() - Comprehensive model evaluation\n",
      "   â€¢ test_fine_tuned_model() - Test model with medical Q&A examples\n",
      "   â€¢ save_trained_model() - Save model with proper organization\n",
      "   â€¢ create_evaluation_report() - Generate evaluation report\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation Functions\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def evaluate_trained_model(trainer, test_dataset, output_dir=None):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"EVALUATING TRAINED MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if trainer is None:\n",
    "        print(\"No trained model available for evaluation\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get evaluation metrics\n",
    "        print(\"Running evaluation on test dataset...\")\n",
    "        eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        \n",
    "        print(f\"\\nEvaluation Results:\")\n",
    "        for metric, value in eval_results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"   {metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"   {metric}: {value}\")\n",
    "        \n",
    "        # Calculate perplexity from eval loss\n",
    "        if 'eval_loss' in eval_results:\n",
    "            perplexity = torch.exp(torch.tensor(eval_results['eval_loss']))\n",
    "            print(f\"   Perplexity: {perplexity:.4f}\")\n",
    "        \n",
    "        # Save evaluation results\n",
    "        if output_dir:\n",
    "            eval_file = os.path.join(output_dir, \"evaluation_results.json\")\n",
    "            with open(eval_file, 'w') as f:\n",
    "                # Add perplexity to results\n",
    "                if 'eval_loss' in eval_results:\n",
    "                    eval_results['perplexity'] = float(perplexity)\n",
    "                eval_results['evaluation_date'] = datetime.now().isoformat()\n",
    "                json.dump(eval_results, f, indent=2)\n",
    "            print(f\"Evaluation results saved to {eval_file}\")\n",
    "        \n",
    "        return eval_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def test_fine_tuned_model(model, tokenizer, test_cases=None, max_length=512):\n",
    "    \"\"\"\n",
    "    Test the fine-tuned model with specific medical Q&A examples\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"TESTING FINE-TUNED MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model or tokenizer not available for testing\")\n",
    "        return None\n",
    "    \n",
    "    # Default test cases if none provided\n",
    "    if test_cases is None:\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"input\": \"\"\"### Instruction:\n",
    "You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\n",
    "\n",
    "### Input:\n",
    "Patient Profile:\n",
    "Age: 45, Gender: M\n",
    "BMI: 28.5 kg/mÂ² (Overweight)\n",
    "Blood Pressure: 140/90 mmHg (Stage 1)\n",
    "Heart Rate: 75 bpm (Normal)\n",
    "HbA1c: 6.2% (Prediabetes)\n",
    "LDL Cholesterol: 145 mg/dL (Borderline High)\n",
    "\n",
    "Patient Question: What are my health risks based on my recent test results?\n",
    "\n",
    "### Response:\"\"\",\n",
    "                \"description\": \"High-risk patient with multiple factors\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"\"\"### Instruction:\n",
    "You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\n",
    "\n",
    "### Input:\n",
    "Patient Profile:\n",
    "Age: 32, Gender: F\n",
    "BMI: 22.1 kg/mÂ² (Normal)\n",
    "Blood Pressure: 115/75 mmHg (Normal)\n",
    "Heart Rate: 68 bpm (Normal)\n",
    "\n",
    "Patient Question: What lifestyle changes should I make to improve my health?\n",
    "\n",
    "### Response:\"\"\",\n",
    "                \"description\": \"Healthy patient seeking lifestyle advice\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"\"\"### Instruction:\n",
    "You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\n",
    "\n",
    "### Input:\n",
    "Patient Profile:\n",
    "Age: 58, Gender: F\n",
    "BMI: 31.2 kg/mÂ² (Obese)\n",
    "Blood Pressure: 155/95 mmHg (Stage 2)\n",
    "HbA1c: 7.1% (Diabetes)\n",
    "LDL Cholesterol: 175 mg/dL (High)\n",
    "\n",
    "Patient Question: Should I be concerned about my cholesterol levels?\n",
    "\n",
    "### Response:\"\"\",\n",
    "                \"description\": \"High-risk diabetic patient\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    try:\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\" Testing with {len(test_cases)} test cases...\")\n",
    "        \n",
    "        test_results = []\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases):\n",
    "            print(f\"\\n Test Case {i+1}: {test_case['description']}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                test_case['input'],\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Move to same device as model\n",
    "            device = next(model.parameters()).device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_new_tokens=200,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            input_length = inputs['input_ids'].shape[1]\n",
    "            generated_tokens = outputs[0][input_length:]\n",
    "            response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            print(f\"Input: {test_case['input'][:100]}...\")\n",
    "            print(f\"Generated Response: {response[:300]}...\")\n",
    "            \n",
    "            test_results.append({\n",
    "                'test_case': i+1,\n",
    "                'description': test_case['description'],\n",
    "                'input': test_case['input'],\n",
    "                'generated_response': response,\n",
    "                'response_length': len(response.split()),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        print(f\"\\n Testing completed! Generated {len(test_results)} responses\")\n",
    "        return test_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Testing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def save_trained_model(trainer, tokenizer, output_dir, model_name=\"medical_llm_qlora\"):\n",
    "    \"\"\"\n",
    "    Save the trained model with proper organization and metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" Saving trained model\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if trainer is None or tokenizer is None:\n",
    "        print(\" Model or tokenizer not available for saving\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create organized directory structure\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_dir = os.path.join(output_dir, f\"{model_name}_{timestamp}\")\n",
    "        \n",
    "        # Create subdirectories\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(model_dir, \"model\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(model_dir, \"tokenizer\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(model_dir, \"metadata\"), exist_ok=True)\n",
    "        \n",
    "        print(f\" Creating model directory: {model_dir}\")\n",
    "        \n",
    "        # Save the model\n",
    "        print(\" Saving model...\")\n",
    "        trainer.save_model(os.path.join(model_dir, \"model\"))\n",
    "        \n",
    "        # Save the tokenizer\n",
    "        print(\" Saving tokenizer...\")\n",
    "        tokenizer.save_pretrained(os.path.join(model_dir, \"tokenizer\"))\n",
    "        \n",
    "        # Save training metadata\n",
    "        print(\" Saving metadata...\")\n",
    "        metadata = {\n",
    "            \"model_name\": model_name,\n",
    "            \"base_model\": MODEL_NAME,\n",
    "            \"training_date\": timestamp,\n",
    "            \"framework\": \"QLoRA\",\n",
    "            \"peft_config\": {\n",
    "                \"r\": peft_config.r,\n",
    "                \"lora_alpha\": peft_config.lora_alpha,\n",
    "                \"lora_dropout\": peft_config.lora_dropout,\n",
    "                \"target_modules\": peft_config.target_modules\n",
    "            },\n",
    "            \"dataset_info\": {\n",
    "                \"train_samples\": len(train_dataset),\n",
    "                \"val_samples\": len(val_dataset),\n",
    "                \"total_samples\": len(train_dataset) + len(val_dataset)\n",
    "            },\n",
    "            \"training_summary\": training_summary if 'training_summary' in globals() else \"Not available\"\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(model_dir, \"metadata\", \"model_info.json\"), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        # Create a README file\n",
    "        readme_content = f\"\"\"# {model_name}\n",
    "\n",
    "## Model Information\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Training Method**: QLoRA (Quantized Low-Rank Adaptation)\n",
    "- **Training Date**: {timestamp}\n",
    "- **Task**: Medical Q&A Fine-tuning\n",
    "\n",
    "## Directory Structure\n",
    "- `model/`: Contains the fine-tuned model files\n",
    "- `tokenizer/`: Contains the tokenizer files\n",
    "- `metadata/`: Contains training metadata and configuration\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tokenizer\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"{MODEL_NAME}\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "model = PeftModel.from_pretrained(base_model, \"./model\")\n",
    "\n",
    "# Generate text\n",
    "inputs = tokenizer(\"Your medical question here\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "- Training Samples: {len(train_dataset)}\n",
    "- Validation Samples: {len(val_dataset)}\n",
    "- LoRA Rank (r): {peft_config.r}\n",
    "- LoRA Alpha: {peft_config.lora_alpha}\n",
    "- Target Modules: {peft_config.target_modules}\n",
    "\n",
    "## Disclaimer\n",
    "This model is for educational and research purposes only. Always consult with qualified healthcare professionals for medical advice.\n",
    "\"\"\"\n",
    "        \n",
    "        with open(os.path.join(model_dir, \"README.md\"), 'w') as f:\n",
    "            f.write(readme_content)\n",
    "        \n",
    "        print(f\"âœ… Model saved successfully!\")\n",
    "        print(f\"ðŸ“ Location: {model_dir}\")\n",
    "        print(f\"ðŸ“Š Model size: {get_directory_size(model_dir):.2f} MB\")\n",
    "        print(f\"ðŸ“ Files saved:\")\n",
    "        print(f\"   â€¢ Model: {os.path.join(model_dir, 'model')}\")\n",
    "        print(f\"   â€¢ Tokenizer: {os.path.join(model_dir, 'tokenizer')}\")\n",
    "        print(f\"   â€¢ Metadata: {os.path.join(model_dir, 'metadata')}\")\n",
    "        print(f\"   â€¢ README.md: {os.path.join(model_dir, 'README.md')}\")\n",
    "        \n",
    "        return model_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model saving failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def get_directory_size(path):\n",
    "    \"\"\"Calculate directory size in MB\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                total_size += os.path.getsize(filepath)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "def create_evaluation_report(eval_results, test_results, model_dir):\n",
    "    \"\"\"Create a comprehensive evaluation report\"\"\"\n",
    "    \n",
    "    print(\"ðŸ“Š CREATING EVALUATION REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        report = {\n",
    "            \"evaluation_summary\": {\n",
    "                \"evaluation_date\": datetime.now().isoformat(),\n",
    "                \"model_performance\": eval_results if eval_results else \"Not available\",\n",
    "                \"test_cases_completed\": len(test_results) if test_results else 0\n",
    "            },\n",
    "            \"detailed_test_results\": test_results if test_results else [],\n",
    "            \"recommendations\": [\n",
    "                \"Model shows capability in medical Q&A tasks\",\n",
    "                \"Responses should be reviewed by medical professionals\",\n",
    "                \"Consider additional fine-tuning for specific medical domains\",\n",
    "                \"Regular evaluation with updated medical guidelines recommended\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        if model_dir:\n",
    "            report_file = os.path.join(model_dir, \"metadata\", \"evaluation_report.json\")\n",
    "            with open(report_file, 'w') as f:\n",
    "                json.dump(report, f, indent=2, default=str)\n",
    "            print(f\"ðŸ“‹ Evaluation report saved to {report_file}\")\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Report creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Model evaluation functions ready!\")\n",
    "print(\"ðŸ“‹ Available functions:\")\n",
    "print(\"   â€¢ evaluate_trained_model() - Comprehensive model evaluation\")\n",
    "print(\"   â€¢ test_fine_tuned_model() - Test model with medical Q&A examples\")  \n",
    "print(\"   â€¢ save_trained_model() - Save model with proper organization\")\n",
    "print(\"   â€¢ create_evaluation_report() - Generate evaluation report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acae7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running FIXED model evaluation...\n",
      "ðŸ§ª TESTING FINE-TUNED MODEL - FIXED VERSION\n",
      "==================================================\n",
      "ðŸ“Š Model Status:\n",
      "   âœ… Model loaded: PeftModelForCausalLM\n",
      "   âœ… Tokenizer available: PreTrainedTokenizerFast\n",
      "   âœ… Device: cuda:0\n",
      "\n",
      "ðŸ” Testing with 3 test cases...\n",
      "\n",
      "ðŸ“‹ Test Case 1: risk_assessment\n",
      "----------------------------------------\n",
      "âœ… Generated Response:\n",
      "   Based on your health profile, here are my observations and recommendations:\\n\\n**Current Risk Factors:** You have excess weight. These factors can increase your risk of cardiovascular disease and othe...\n",
      "ðŸ“ Response Length: 593 characters\n",
      "\n",
      "ðŸ“‹ Test Case 2: specific_concerns\n",
      "----------------------------------------\n",
      "âœ… Generated Response:\n",
      "   Based on your health profile, here are my observations and recommendations:\\n\\n**Current Risk Factors:** You have excess weight. These factors can increase your risk of cardiovascular disease and othe...\n",
      "ðŸ“ Response Length: 593 characters\n",
      "\n",
      "ðŸ“‹ Test Case 2: specific_concerns\n",
      "----------------------------------------\n",
      "âœ… Generated Response:\n",
      "   Based on your health profile, here are my observations and recommendations:\\n\\n**Current Risk Factors:** You have excess weight. These factors can increase your risk of cardiovascular disease and othe...\n",
      "ðŸ“ Response Length: 593 characters\n",
      "\n",
      "ðŸ“‹ Test Case 3: lifestyle_advice\n",
      "----------------------------------------\n",
      "âœ… Generated Response:\n",
      "   Based on your health profile, here are my observations and recommendations:\\n\\n**Current Risk Factors:** You have excess weight. These factors can increase your risk of cardiovascular disease and othe...\n",
      "ðŸ“ Response Length: 593 characters\n",
      "\n",
      "ðŸ“‹ Test Case 3: lifestyle_advice\n",
      "----------------------------------------\n",
      "âœ… Generated Response:\n",
      "   ...\n",
      "ðŸ“ Response Length: 0 characters\n",
      "\n",
      "ðŸ“Š EVALUATION SUMMARY:\n",
      "   âœ… Successful tests: 3/3\n",
      "   ðŸ“ˆ Success rate: 100.0%\n",
      "   ðŸ“ Average response length: 395 characters\n",
      "   âœ… Model is generating coherent responses!\n",
      "\n",
      "âœ… Evaluation completed successfully!\n",
      "\n",
      "ðŸ©º MEDICAL SCENARIO TESTING\n",
      "========================================\n",
      "\n",
      "ðŸŽ¯ Scenario 1: High Blood Pressure\n",
      "------------------------------\n",
      "âœ… Generated Response:\n",
      "   ...\n",
      "ðŸ“ Response Length: 0 characters\n",
      "\n",
      "ðŸ“Š EVALUATION SUMMARY:\n",
      "   âœ… Successful tests: 3/3\n",
      "   ðŸ“ˆ Success rate: 100.0%\n",
      "   ðŸ“ Average response length: 395 characters\n",
      "   âœ… Model is generating coherent responses!\n",
      "\n",
      "âœ… Evaluation completed successfully!\n",
      "\n",
      "ðŸ©º MEDICAL SCENARIO TESTING\n",
      "========================================\n",
      "\n",
      "ðŸŽ¯ Scenario 1: High Blood Pressure\n",
      "------------------------------\n",
      "ðŸ¤– Model Response:\n",
      "   I recommend making lifestyle changes and considering medication based on your overall health status and my observations. Here are my recommendations:\n",
      "\n",
      "Current Risk Factors: Elevated blood pressure, excess weight. These factors can increase your risk of cardiovascular disease and other complications.\n",
      "\n",
      "Lifestyle Recommendations:\n",
      "â€¢ Focus on reducing sodium intake and increasing physical activity to help manage blood pressure\n",
      "â€¢ Consider a structured weight management program with balanced nutrition and regular exercise\n",
      "â€¢ Regular monitoring and follow-up with your healthcare provider is important\n",
      "\n",
      "Consideration for Medication: Given your blood pressure reading, my clinical judgment is that lifestyle changes are important but not sufficient based on my observations. Consideration for medication should be discussed with your healthcare provider.\n",
      "\n",
      "Important Note: This guidance is for educational purposes. Please consult with your healthcare provider for personalized medical advice and treatment decisions.\n",
      "\n",
      "ðŸŽ¯ Scenario 2: Diabetes Risk\n",
      "------------------------------\n",
      "ðŸ¤– Model Response:\n",
      "   I recommend making lifestyle changes and considering medication based on your overall health status and my observations. Here are my recommendations:\n",
      "\n",
      "Current Risk Factors: Elevated blood pressure, excess weight. These factors can increase your risk of cardiovascular disease and other complications.\n",
      "\n",
      "Lifestyle Recommendations:\n",
      "â€¢ Focus on reducing sodium intake and increasing physical activity to help manage blood pressure\n",
      "â€¢ Consider a structured weight management program with balanced nutrition and regular exercise\n",
      "â€¢ Regular monitoring and follow-up with your healthcare provider is important\n",
      "\n",
      "Consideration for Medication: Given your blood pressure reading, my clinical judgment is that lifestyle changes are important but not sufficient based on my observations. Consideration for medication should be discussed with your healthcare provider.\n",
      "\n",
      "Important Note: This guidance is for educational purposes. Please consult with your healthcare provider for personalized medical advice and treatment decisions.\n",
      "\n",
      "ðŸŽ¯ Scenario 2: Diabetes Risk\n",
      "------------------------------\n",
      "ðŸ¤– Model Response:\n",
      "   Based on your health profile, here are my observations and recommendations:\n",
      "\n",
      "You have elevated blood glucose levels indicating prediabetes. Additionally, you have excess weight. These factors can increase your risk of developing diabetes and other cardiovascular diseases.\n",
      "\n",
      "To reduce your risk and improve your health, consider the following lifestyle changes recommendations:\n",
      "\n",
      "- Focus on reducing sodium intake and increasing physical activity to help manage blood pressure\n",
      "- Consider a structured weight management program with balanced nutrition and regular exercise\n",
      "- Regular monitoring and follow-up with your healthcare provider is important\n",
      "\n",
      "Remember, these are general observations and recommendations. Always consult with your healthcare provider for personalized medical advice and treatment decisions.\n",
      "\n",
      "ðŸŽ¯ Scenario 3: Heart Rate Concern\n",
      "------------------------------\n",
      "ðŸ¤– Model Response:\n",
      "   Based on your health profile, here are my observations and recommendations:\n",
      "\n",
      "You have elevated blood glucose levels indicating prediabetes. Additionally, you have excess weight. These factors can increase your risk of developing diabetes and other cardiovascular diseases.\n",
      "\n",
      "To reduce your risk and improve your health, consider the following lifestyle changes recommendations:\n",
      "\n",
      "- Focus on reducing sodium intake and increasing physical activity to help manage blood pressure\n",
      "- Consider a structured weight management program with balanced nutrition and regular exercise\n",
      "- Regular monitoring and follow-up with your healthcare provider is important\n",
      "\n",
      "Remember, these are general observations and recommendations. Always consult with your healthcare provider for personalized medical advice and treatment decisions.\n",
      "\n",
      "ðŸŽ¯ Scenario 3: Heart Rate Concern\n",
      "------------------------------\n",
      "ðŸ¤– Model Response:\n",
      "   I'm not able to provide real-time medical guidance. However, here are some general educational points for you to share with the patient:\n",
      "\n",
      "Heart rate can be affected by various factors such as stress, exercise, medication, and underlying health conditions. It's important to discuss any concerns or observations with your healthcare provider. They will be able to assess your overall health and provide personalized guidance based on your medical history and current health status.\n",
      "\n",
      "ðŸ’¾ Evaluation results saved to: model_evaluation_results.json\n",
      "ðŸŽ‰ MODEL TESTING COMPLETED SUCCESSFULLY!\n",
      "ðŸ¤– Model Response:\n",
      "   I'm not able to provide real-time medical guidance. However, here are some general educational points for you to share with the patient:\n",
      "\n",
      "Heart rate can be affected by various factors such as stress, exercise, medication, and underlying health conditions. It's important to discuss any concerns or observations with your healthcare provider. They will be able to assess your overall health and provide personalized guidance based on your medical history and current health status.\n",
      "\n",
      "ðŸ’¾ Evaluation results saved to: model_evaluation_results.json\n",
      "ðŸŽ‰ MODEL TESTING COMPLETED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "# FIXED MODEL EVALUATION AND TESTING\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_trained_model_fixed():\n",
    "    \"\"\"FIXED: Evaluate the trained model using direct inference\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª TESTING FINE-TUNED MODEL - FIXED VERSION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not hasattr(model, 'generate'):\n",
    "        print(\"âŒ Model not available for evaluation\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Ensure model is in eval mode\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"ðŸ“Š Model Status:\")\n",
    "        print(f\"   âœ… Model loaded: {type(model).__name__}\")\n",
    "        print(f\"   âœ… Tokenizer available: {type(tokenizer).__name__}\")\n",
    "        print(f\"   âœ… Device: {next(model.parameters()).device}\")\n",
    "        \n",
    "        # Test with a few examples from our test split\n",
    "        if len(test_split) == 0:\n",
    "            print(\"âš ï¸ No test data available - creating synthetic test\")\n",
    "            return None\n",
    "            \n",
    "        num_test_cases = min(3, len(test_split))\n",
    "        print(f\"\\nðŸ” Testing with {num_test_cases} test cases...\")\n",
    "        \n",
    "        test_results = []\n",
    "        \n",
    "        for i in range(num_test_cases):\n",
    "            test_case = test_split[i]\n",
    "            \n",
    "            print(f\"\\nðŸ“‹ Test Case {i+1}: {test_case.get('question_type', 'Unknown type')}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Create the full prompt\n",
    "            test_prompt = f\"\"\"### Instruction:\n",
    "{test_case['instruction']}\n",
    "\n",
    "### Input:\n",
    "{test_case['input']}\n",
    "\n",
    "### Response:\"\"\"\n",
    "            \n",
    "            try:\n",
    "                # Tokenize input\n",
    "                inputs = tokenizer(\n",
    "                    test_prompt, \n",
    "                    return_tensors=\"pt\", \n",
    "                    truncation=True, \n",
    "                    max_length=512,\n",
    "                    padding=False\n",
    "                )\n",
    "                \n",
    "                # Move to correct device if needed\n",
    "                device = next(model.parameters()).device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Generate response\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=150,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                # Decode the response\n",
    "                full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Extract only the generated part\n",
    "                generated_response = full_response[len(test_prompt):].strip()\n",
    "                \n",
    "                # Store results\n",
    "                test_result = {\n",
    "                    'test_case': i + 1,\n",
    "                    'question_type': test_case.get('question_type', 'Unknown'),\n",
    "                    'input_preview': test_case['input'][:100] + '...',\n",
    "                    'generated_response': generated_response,\n",
    "                    'expected_response': test_case['output'][:100] + '...',\n",
    "                    'response_length': len(generated_response)\n",
    "                }\n",
    "                \n",
    "                test_results.append(test_result)\n",
    "                \n",
    "                # Print results\n",
    "                print(f\"âœ… Generated Response:\")\n",
    "                print(f\"   {generated_response[:200]}...\")\n",
    "                print(f\"ðŸ“ Response Length: {len(generated_response)} characters\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Test case {i+1} failed: {e}\")\n",
    "                test_results.append({\n",
    "                    'test_case': i + 1,\n",
    "                    'error': str(e),\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "        \n",
    "        # Summary\n",
    "        successful_tests = len([r for r in test_results if 'error' not in r])\n",
    "        \n",
    "        print(f\"\\nðŸ“Š EVALUATION SUMMARY:\")\n",
    "        print(f\"   âœ… Successful tests: {successful_tests}/{num_test_cases}\")\n",
    "        print(f\"   ðŸ“ˆ Success rate: {(successful_tests/num_test_cases)*100:.1f}%\")\n",
    "        \n",
    "        if successful_tests > 0:\n",
    "            avg_length = np.mean([r.get('response_length', 0) for r in test_results if 'response_length' in r])\n",
    "            print(f\"   ðŸ“ Average response length: {avg_length:.0f} characters\")\n",
    "            print(\"   âœ… Model is generating coherent responses!\")\n",
    "        else:\n",
    "            print(\"   âŒ All tests failed - model may have issues\")\n",
    "        \n",
    "        return {\n",
    "            'evaluation_completed': True,\n",
    "            'test_results': test_results,\n",
    "            'success_rate': (successful_tests/num_test_cases)*100,\n",
    "            'total_tests': num_test_cases,\n",
    "            'successful_tests': successful_tests,\n",
    "            'evaluation_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def test_specific_medical_scenarios():\n",
    "    \"\"\"Test the model with specific medical scenarios\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ©º MEDICAL SCENARIO TESTING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Define specific medical test scenarios\n",
    "    medical_scenarios = [\n",
    "        {\n",
    "            \"scenario\": \"High Blood Pressure\",\n",
    "            \"patient_info\": \"Age: 45, Gender: Male\\nBlood Pressure: 150/95 mmHg (Stage 1)\\nBMI: 28.5 kg/mÂ² (Overweight)\",\n",
    "            \"question\": \"What should I do about my elevated blood pressure?\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Diabetes Risk\",\n",
    "            \"patient_info\": \"Age: 52, Gender: Female\\nHbA1c: 6.2% (Prediabetes)\\nBMI: 32.1 kg/mÂ² (Obese)\",\n",
    "            \"question\": \"Am I at risk for diabetes and what can I do?\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Heart Rate Concern\",\n",
    "            \"patient_info\": \"Age: 38, Gender: Female\\nHeart Rate: 105 bpm (Tachycardia)\\nBP: 125/80 mmHg (Normal)\",\n",
    "            \"question\": \"Is my heart rate something to worry about?\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    scenario_results = []\n",
    "    \n",
    "    for i, scenario in enumerate(medical_scenarios):\n",
    "        print(f\"\\nðŸŽ¯ Scenario {i+1}: {scenario['scenario']}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Create medical prompt\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\n",
    "\n",
    "### Input:\n",
    "{scenario['patient_info']}\n",
    "\n",
    "Patient Question: {scenario['question']}\n",
    "\n",
    "### Response:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            device = next(model.parameters()).device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=200,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generated = response[len(prompt):].strip()\n",
    "            \n",
    "            print(f\"ðŸ¤– Model Response:\")\n",
    "            print(f\"   {generated}\")\n",
    "            \n",
    "            scenario_results.append({\n",
    "                'scenario': scenario['scenario'],\n",
    "                'response': generated,\n",
    "                'status': 'success'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Scenario failed: {e}\")\n",
    "            scenario_results.append({\n",
    "                'scenario': scenario['scenario'],\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            })\n",
    "    \n",
    "    return scenario_results\n",
    "\n",
    "# Run the FIXED evaluation\n",
    "print(\"ðŸš€ Running FIXED model evaluation...\")\n",
    "eval_results = evaluate_trained_model_fixed()\n",
    "\n",
    "if eval_results:\n",
    "    print(\"\\nâœ… Evaluation completed successfully!\")\n",
    "    \n",
    "    # Run medical scenarios test\n",
    "    scenario_results = test_specific_medical_scenarios()\n",
    "    \n",
    "    # Combine results\n",
    "    final_results = {\n",
    "        'general_evaluation': eval_results,\n",
    "        'medical_scenarios': scenario_results,\n",
    "        'overall_status': 'completed',\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    with open('model_evaluation_results.json', 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Evaluation results saved to: model_evaluation_results.json\")\n",
    "    print(\"ðŸŽ‰ MODEL TESTING COMPLETED SUCCESSFULLY!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Evaluation failed - check errors above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8fa1215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Executing comprehensive model saving...\n",
      "ðŸ’¾ COMPREHENSIVE MODEL SAVING\n",
      "========================================\n",
      "ðŸ“ Created save directories:\n",
      "   lora_adapter: ./medical_model_lora_20251204_034311\n",
      "   merged_model: ./medical_model_merged_20251204_034311\n",
      "   production: ./medical_model_production_20251204_034311\n",
      "\n",
      "ðŸ’¿ Saving LoRA adapter...\n",
      "   âœ… LoRA adapter saved to: ./medical_model_lora_20251204_034311\n",
      "\n",
      "ðŸ”„ Merging and saving full model...\n",
      "   âœ… LoRA adapter saved to: ./medical_model_lora_20251204_034311\n",
      "\n",
      "ðŸ”„ Merging and saving full model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Merged model saved to: ./medical_model_merged_20251204_034311\n",
      "\n",
      "ðŸš€ Creating production package...\n",
      "   âœ… Production package saved to: ./medical_model_production_20251204_034311\n",
      "   ðŸ“‹ Includes README.md and metadata.json\n",
      "\n",
      "ðŸ“Š SAVE SUMMARY:\n",
      "   âœ… LoRA adapter: ./medical_model_lora_20251204_034311\n",
      "   âœ… Merged model: ./medical_model_merged_20251204_034311\n",
      "   âœ… Production ready: ./medical_model_production_20251204_034311\n",
      "   ðŸ“„ Training summary: training_complete_summary_20251204_034311.json\n",
      "   âœ… Production package saved to: ./medical_model_production_20251204_034311\n",
      "   ðŸ“‹ Includes README.md and metadata.json\n",
      "\n",
      "ðŸ“Š SAVE SUMMARY:\n",
      "   âœ… LoRA adapter: ./medical_model_lora_20251204_034311\n",
      "   âœ… Merged model: ./medical_model_merged_20251204_034311\n",
      "   âœ… Production ready: ./medical_model_production_20251204_034311\n",
      "   ðŸ“„ Training summary: training_complete_summary_20251204_034311.json\n"
     ]
    }
   ],
   "source": [
    "# ENHANCED MODEL SAVING WITH MULTIPLE FORMATS\n",
    "def save_trained_model_comprehensive():\n",
    "    \"\"\"Save the trained model in multiple formats for different use cases\"\"\"\n",
    "    \n",
    "    print(\"ðŸ’¾ COMPREHENSIVE MODEL SAVING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create different save directories for different purposes\n",
    "        save_dirs = {\n",
    "            'lora_adapter': f'./medical_model_lora_{timestamp}',\n",
    "            'merged_model': f'./medical_model_merged_{timestamp}',\n",
    "            'production': f'./medical_model_production_{timestamp}'\n",
    "        }\n",
    "        \n",
    "        for save_type, save_dir in save_dirs.items():\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"ðŸ“ Created save directories:\")\n",
    "        for save_type, save_dir in save_dirs.items():\n",
    "            print(f\"   {save_type}: {save_dir}\")\n",
    "        \n",
    "        # 1. Save LoRA adapter only (smallest size)\n",
    "        print(\"\\nðŸ’¿ Saving LoRA adapter...\")\n",
    "        try:\n",
    "            model.save_pretrained(save_dirs['lora_adapter'])\n",
    "            tokenizer.save_pretrained(save_dirs['lora_adapter'])\n",
    "            print(f\"   âœ… LoRA adapter saved to: {save_dirs['lora_adapter']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ LoRA save failed: {e}\")\n",
    "        \n",
    "        # 2. Save merged model (full model with LoRA weights merged)\n",
    "        print(\"\\nðŸ”„ Merging and saving full model...\")\n",
    "        try:\n",
    "            merged_model = model.merge_and_unload()\n",
    "            merged_model.save_pretrained(save_dirs['merged_model'])\n",
    "            tokenizer.save_pretrained(save_dirs['merged_model'])\n",
    "            print(f\"   âœ… Merged model saved to: {save_dirs['merged_model']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Merged model save failed: {e}\")\n",
    "            merged_model = None\n",
    "        \n",
    "        # 3. Save production-ready package\n",
    "        print(\"\\nðŸš€ Creating production package...\")\n",
    "        try:\n",
    "            # Use merged model if available, otherwise use current model\n",
    "            production_model = merged_model if merged_model else model\n",
    "            \n",
    "            production_model.save_pretrained(save_dirs['production'])\n",
    "            tokenizer.save_pretrained(save_dirs['production'])\n",
    "            \n",
    "            # Create model card and usage instructions\n",
    "            model_card = f\"\"\"# Medical AI Assistant Model\n",
    "\n",
    "## Model Information\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Fine-tuning Method**: QLoRA (Low-Rank Adaptation)\n",
    "- **Training Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "- **Use Case**: Medical question answering and health guidance\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{save_dirs['production']}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{save_dirs['production']}\")\n",
    "\n",
    "# Generate medical advice\n",
    "def get_medical_guidance(patient_info, question):\n",
    "    prompt = f'''### Instruction:\n",
    "You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\n",
    "\n",
    "### Input:\n",
    "{{patient_info}}\n",
    "\n",
    "Patient Question: {{question}}\n",
    "\n",
    "### Response:'''\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "```\n",
    "\n",
    "## Important Disclaimers\n",
    "- This model is for educational and informational purposes only\n",
    "- Always consult healthcare professionals for medical decisions\n",
    "- Do not use for emergency medical situations\n",
    "\"\"\"\n",
    "            \n",
    "            with open(f\"{save_dirs['production']}/README.md\", 'w') as f:\n",
    "                f.write(model_card)\n",
    "            \n",
    "            # Save training metadata\n",
    "            metadata = {\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"fine_tuning_method\": \"QLoRA\",\n",
    "                \"training_timestamp\": timestamp,\n",
    "                \"training_data_size\": len(train_split),\n",
    "                \"validation_data_size\": len(val_split),\n",
    "                \"test_data_size\": len(test_split),\n",
    "                \"model_type\": \"medical_qa_assistant\",\n",
    "                \"save_directories\": save_dirs,\n",
    "                \"model_ready\": True\n",
    "            }\n",
    "            \n",
    "            with open(f\"{save_dirs['production']}/model_metadata.json\", 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            \n",
    "            print(f\"   âœ… Production package saved to: {save_dirs['production']}\")\n",
    "            print(f\"   ðŸ“‹ Includes README.md and metadata.json\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Production package save failed: {e}\")\n",
    "        \n",
    "        # 4. Save training summary\n",
    "        training_summary_final = {\n",
    "            \"training_completed\": True,\n",
    "            \"model_saved\": True,\n",
    "            \"save_directories\": save_dirs,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"total_training_examples\": len(train_split),\n",
    "            \"ready_for_use\": True\n",
    "        }\n",
    "        \n",
    "        with open(f'./training_complete_summary_{timestamp}.json', 'w') as f:\n",
    "            json.dump(training_summary_final, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š SAVE SUMMARY:\")\n",
    "        print(f\"   âœ… LoRA adapter: {save_dirs['lora_adapter']}\")\n",
    "        print(f\"   âœ… Merged model: {save_dirs['merged_model']}\")\n",
    "        print(f\"   âœ… Production ready: {save_dirs['production']}\")\n",
    "        print(f\"   ðŸ“„ Training summary: training_complete_summary_{timestamp}.json\")\n",
    "        \n",
    "        return save_dirs, training_summary_final\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model saving failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Execute comprehensive model saving\n",
    "print(\"\\nðŸš€ Executing comprehensive model saving...\")\n",
    "save_directories, save_summary = save_trained_model_comprehensive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f34c9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loading and usage functions ready!\n",
      "ðŸ“‹ Additional functions:\n",
      "   â€¢ load_saved_model() - Load a previously saved model\n",
      "   â€¢ generate_medical_response() - Generate medical responses\n",
      "   â€¢ demo_model_usage() - Demonstrate model usage\n"
     ]
    }
   ],
   "source": [
    "# Model Loading and Usage Functions\n",
    "def load_saved_model(model_directory):\n",
    "    \"\"\"\n",
    "    Load a previously saved fine-tuned model for inference\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸ“¥ LOADING SAVED MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        from peft import PeftModel\n",
    "        \n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(model_directory):\n",
    "            print(f\"âŒ Model directory not found: {model_directory}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer_path = os.path.join(model_directory, \"tokenizer\")\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            print(\"ðŸ“ Loading tokenizer...\")\n",
    "            loaded_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "            print(\"âœ… Tokenizer loaded successfully\")\n",
    "        else:\n",
    "            print(\"âŒ Tokenizer not found\")\n",
    "            return None, None\n",
    "        \n",
    "        # Load model\n",
    "        model_path = os.path.join(model_directory, \"model\")\n",
    "        if os.path.exists(model_path):\n",
    "            print(\"ðŸ¤– Loading fine-tuned model...\")\n",
    "            \n",
    "            # Load base model first\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Load LoRA weights\n",
    "            loaded_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "            print(\"âœ… Model loaded successfully\")\n",
    "        else:\n",
    "            print(\"âŒ Model not found\")\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"ðŸŽ¯ Model ready for inference!\")\n",
    "        return loaded_model, loaded_tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model loading failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def generate_medical_response(model, tokenizer, patient_profile, question, max_new_tokens=200):\n",
    "    \"\"\"\n",
    "    Generate a medical response using the fine-tuned model\n",
    "    \"\"\"\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"âŒ Model or tokenizer not available\")\n",
    "        return None\n",
    "    \n",
    "    # Format the input\n",
    "    formatted_input = f\"\"\"### Instruction:\n",
    "You are a healthcare assistant. Based on the patient's profile and question, provide appropriate medical guidance.\n",
    "\n",
    "### Input:\n",
    "{patient_profile}\n",
    "\n",
    "Patient Question: {question}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            formatted_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        generated_tokens = outputs[0][input_length:]\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Response generation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def demo_model_usage(model_directory=None):\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the saved model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ­ MEDICAL LLM USAGE DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use current model or load from directory\n",
    "    if model_directory:\n",
    "        demo_model, demo_tokenizer = load_saved_model(model_directory)\n",
    "    else:\n",
    "        demo_model = final_trainer.model if final_trainer else None\n",
    "        demo_tokenizer = tokenizer\n",
    "    \n",
    "    if demo_model is None or demo_tokenizer is None:\n",
    "        print(\"âŒ No model available for demonstration\")\n",
    "        return\n",
    "    \n",
    "    # Demo patient profiles and questions\n",
    "    demo_cases = [\n",
    "        {\n",
    "            \"profile\": \"\"\"Patient Profile:\n",
    "Age: 52, Gender: M\n",
    "BMI: 27.8 kg/mÂ² (Overweight)\n",
    "Blood Pressure: 135/85 mmHg (Stage 1)\n",
    "HbA1c: 5.9% (Prediabetes)\n",
    "LDL Cholesterol: 155 mg/dL (Borderline High)\"\"\",\n",
    "            \"question\": \"What steps should I take to prevent diabetes?\"\n",
    "        },\n",
    "        {\n",
    "            \"profile\": \"\"\"Patient Profile:\n",
    "Age: 38, Gender: F\n",
    "BMI: 23.2 kg/mÂ² (Normal)\n",
    "Blood Pressure: 118/78 mmHg (Normal)\n",
    "Heart Rate: 72 bpm (Normal)\"\"\",\n",
    "            \"question\": \"How often should I get health checkups?\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"ðŸ”¬ Testing with {len(demo_cases)} demo cases...\")\n",
    "    \n",
    "    for i, case in enumerate(demo_cases):\n",
    "        print(f\"\\nðŸ“ Demo Case {i+1}:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Profile: {case['profile'][:60]}...\")\n",
    "        print(f\"Question: {case['question']}\")\n",
    "        \n",
    "        response = generate_medical_response(\n",
    "            model=demo_model,\n",
    "            tokenizer=demo_tokenizer,\n",
    "            patient_profile=case['profile'],\n",
    "            question=case['question']\n",
    "        )\n",
    "        \n",
    "        if response:\n",
    "            print(f\"Response: {response[:200]}...\")\n",
    "        else:\n",
    "            print(\"âŒ Failed to generate response\")\n",
    "    \n",
    "    print(f\"\\nâœ… Demonstration complete!\")\n",
    "\n",
    "print(\"âœ… Model loading and usage functions ready!\")\n",
    "print(\"ðŸ“‹ Additional functions:\")\n",
    "print(\"   â€¢ load_saved_model() - Load a previously saved model\")\n",
    "print(\"   â€¢ generate_medical_response() - Generate medical responses\")\n",
    "print(\"   â€¢ demo_model_usage() - Demonstrate model usage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
